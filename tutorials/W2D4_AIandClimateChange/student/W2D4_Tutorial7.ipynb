{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eeed120",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Bonus Tutorial 7: Deep Learning for Climate Prediction with CNN-LSTMs (PyTorch)\n",
    "\n",
    "**Week 2, Day 4, AI and Climate Change**\n",
    "\n",
    "**Content creators:** Deepak Mewada, Grace Lindsay\n",
    "\n",
    "**Content reviewers:**  Jenna Pearson\n",
    "\n",
    "**Content editors:** Deepak Mewada, Grace Lindsay\n",
    "\n",
    "**Production editors:**  Jenna Pearson, Konstantine Tsafatinos\n",
    "\n",
    "**Our 2024 Sponsors:** CMIP, NFDI4Earth\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GMS6-O5IRpKM",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial:* 60 minutes\n",
    "\n",
    "Welcome back! You've skillfully applied `scikit-learn` to climate modeling in [Tutorial 1](link_to_tutorial1) and [Tutorial 2](link_to_tutorial2). Now, get ready to dive into the world of **Deep Learning** using `PyTorch`! This tutorial focuses on a `Convolutional Neural Network (CNN)` combined with a `Long Short-Term Memory (LSTM)` network, a powerful architecture for spatiotemporal data.\n",
    "\n",
    "\n",
    "In this tutorial, you will learn\n",
    "*   Deep Learning Fundamentals\n",
    "*   PyTorch Primer\n",
    "*   Climate Data in Tensors\n",
    "*   Defining the DL model - `CNN-LSTM`\n",
    "*   Training the Model\n",
    "*   Making Prediction from the trained model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YCfpwKbfZCw2",
   "metadata": {
    "execution": {}
   },
   "source": [
    "#Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cvf3Ux3GZCw2",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np # Numerical computing\n",
    "import xarray as xr # Labeled multi-dimensional arrays\n",
    "import pandas as pd # Data analysis and manipulation\n",
    "import cartopy.crs as ccrs # Geospatial plotting\n",
    "import matplotlib.pyplot as plt # Plotting\n",
    "from types import MethodType\n",
    "from IPython.display import clear_output\n",
    "import types\n",
    "\n",
    "import torch # PyTorch!\n",
    "import torch.nn as nn # Neural network layers\n",
    "import torch.optim as optim # Optimization algorithms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import random # Random number generation\n",
    "\n",
    "#from tqdm import tqdm\n",
    "from ipywidgets import interact, IntSlider\n",
    "import plotly.express as px\n",
    "\n",
    "import os, sys, contextlib\n",
    "import pooch\n",
    "from IPython.display import display\n",
    "import logging\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YPGgRQH-Q7r8",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install and import feedback gadget\n",
    "\n",
    "!pip3 install vibecheck datatops --quiet\n",
    "\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "def content_review(notebook_section: str):\n",
    "    return DatatopsContentReviewContainer(\n",
    "        \"\",  # No text prompt\n",
    "        notebook_section,\n",
    "        {\n",
    "            \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "            \"name\": \"comptools_4clim\",\n",
    "            \"user_key\": \"l5jpxuee\",\n",
    "        },\n",
    "    ).render()\n",
    "\n",
    "\n",
    "feedback_prefix = \"W2D4_T7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uLkA23KzRZ_L",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "import ipywidgets as widgets  # interactive display\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\n",
    "    \"https://raw.githubusercontent.com/neuromatch/climate-course-content/main/cma.mplstyle\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jGzF_XKCbrDJ",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title  Data retrieval helper function\n",
    "\n",
    "# Helper functions to download selected climate scenarios and test data (hidden setup)\n",
    "\n",
    "# Silence pooch warnings (e.g., SHA256 hash prints)\n",
    "import logging\n",
    "logging.getLogger(\"pooch\").setLevel(logging.CRITICAL)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Scenario-to-OSF code mapping\n",
    "scenario_code_map = {\n",
    "    'ssp126': ('jvqg5', '9jmsy'),\n",
    "    #'ssp245': ('hqvkz', 'k7fqu'),\n",
    "    'ssp370': ('4snxb', 'zcafm'),\n",
    "    'ssp585': ('sejxt', 'vwg39'),\n",
    "    'hist-GHG': ('p84hg', 'ys7nu'),\n",
    "    'hist-aer': ('q7skr', 'bq3k8'),\n",
    "    'historical': ('kqxet', 'une23')\n",
    "}\n",
    "\n",
    "osf_base_url = \"https://osf.io/download/\"\n",
    "\n",
    "# Suppress printing (used in background downloads)\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def suppress_output():\n",
    "    # Save the current file descriptors\n",
    "    devnull = os.open(os.devnull, os.O_WRONLY)\n",
    "    old_stdout = os.dup(1)\n",
    "    old_stderr = os.dup(2)\n",
    "\n",
    "    try:\n",
    "        # Redirect stdout and stderr to /dev/null\n",
    "        os.dup2(devnull, 1)\n",
    "        os.dup2(devnull, 2)\n",
    "        yield\n",
    "    finally:\n",
    "        # Restore original stdout and stderr\n",
    "        os.dup2(old_stdout, 1)\n",
    "        os.dup2(old_stderr, 2)\n",
    "        os.close(devnull)\n",
    "        os.close(old_stdout)\n",
    "        os.close(old_stderr)\n",
    "\n",
    "# Function to download train/val files for selected scenarios\n",
    "def download_selected_scenarios(selected_climate_input_vars, train_val_dir=\"Data/train_val/\"):\n",
    "    os.makedirs(train_val_dir, exist_ok=True)\n",
    "    file_map = {\n",
    "         'inputs_historical.nc': scenario_code_map['historical'][0],\n",
    "        'outputs_historical.nc': scenario_code_map['historical'][1]\n",
    "    }\n",
    "\n",
    "    for s in selected_climate_input_vars:\n",
    "        code_in, code_out = scenario_code_map[s]\n",
    "        file_map[f'inputs_{s}.nc'] = code_in\n",
    "        file_map[f'outputs_{s}.nc'] = code_out\n",
    "\n",
    "    with suppress_output():\n",
    "        for fname, code in file_map.items():\n",
    "            url = osf_base_url + code + \"/\"\n",
    "            _ = pooch.retrieve(url=url, known_hash=None, fname=fname, path=train_val_dir, progressbar=False)\n",
    "\n",
    "# Function to download test files (must be called explicitly)\n",
    "def download_test_data(test_dir=\"Data/test/\"):\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    file_map = {\n",
    "        'inputs_ssp245.nc': '8gpvw',\n",
    "        'outputs_ssp245.nc': '9pmtx'\n",
    "    }\n",
    "    with suppress_output():\n",
    "        for fname, code in file_map.items():\n",
    "            url = osf_base_url + code + \"/\"\n",
    "            _ = pooch.retrieve(url=url, known_hash=None, fname=fname, path=test_dir, progressbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AZobH8uCZCw3",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Helper functions  {\"run\":\"auto\",\"display-mode\":\"form\"}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML, display, clear_output\n",
    "from ipywidgets import interact\n",
    "import warnings\n",
    "from matplotlib.colors import LogNorm\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*FrozenMappingWarningOnValuesAccess.*\")\n",
    "# Ensure interactive mode\n",
    "%matplotlib inline\n",
    "\n",
    "flag=0 #to be used in section 10.1\n",
    "\n",
    "\n",
    "# Helper function for climate scenario selector\n",
    "def setup_scenario_selector():\n",
    "    # Available scenarios\n",
    "    available_scenarios = ['historical','ssp126',  'ssp370', 'ssp585', 'hist-GHG', 'hist-aer']\n",
    "    all_scenarios_set = set(available_scenarios)\n",
    "\n",
    "    # Declare global to store selected scenarios\n",
    "    global scenario_selected\n",
    "    scenario_selected = ['historical']  # Default\n",
    "\n",
    "    # Dropdown widget\n",
    "    selector = widgets.SelectMultiple(\n",
    "        options=available_scenarios,\n",
    "        value=tuple(scenario_selected),\n",
    "        description='Scenarios:',\n",
    "        rows=len(available_scenarios),\n",
    "        layout=widgets.Layout(width='50%'),\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    # Output area\n",
    "    output = widgets.Output()\n",
    "\n",
    "    # Callback on selection change\n",
    "    def on_change(change):\n",
    "        global scenario_selected\n",
    "        scenario_selected = list(selector.value)\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Selected scenarios: {scenario_selected}\")\n",
    "            if set(scenario_selected) != all_scenarios_set:\n",
    "                missing = sorted(all_scenarios_set - set(scenario_selected))\n",
    "                print(f\"Tip: You did not select: {missing}\")\n",
    "                print(\"For better model performance, select **all** scenarios.\")\n",
    "        # Call downstream logic (if defined)\n",
    "        download_selected_scenarios(scenario_selected)\n",
    "\n",
    "    # Attach observer\n",
    "    selector.observe(on_change, names='value')\n",
    "\n",
    "    # Initial display\n",
    "    on_change(None)\n",
    "    display(selector, output)\n",
    "\n",
    "\n",
    "\n",
    "def setup_climate_input_selector():\n",
    "    # Make accessible outside the function\n",
    "    global selected_climate_input_vars\n",
    "\n",
    "    climate_vars = ['CO2', 'CH4', 'SO2', 'BC']\n",
    "\n",
    "    dropdown = widgets.SelectMultiple(\n",
    "        options=climate_vars,\n",
    "        value=('CO2',),  # Default selection\n",
    "        description='Inputs:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='50%'),\n",
    "    )\n",
    "\n",
    "    output = widgets.Output()\n",
    "\n",
    "    def on_change(change):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            selected = list(dropdown.value)\n",
    "            missed = list(set(climate_vars) - set(selected))\n",
    "\n",
    "            print(f\"Selected input variables: {selected}\")\n",
    "            if missed:\n",
    "                print(f\"⚠️ You missed: {missed}\")\n",
    "                print(\"🔎 Tip: For best model performance, please select **all** input variables.\")\n",
    "            else:\n",
    "                print(\"✅ All input variables selected. Great for training!\")\n",
    "\n",
    "        # Update global variable\n",
    "        global selected_climate_input_vars\n",
    "        selected_climate_input_vars = selected\n",
    "\n",
    "    dropdown.observe(on_change, names='value')\n",
    "\n",
    "    display(dropdown, output)\n",
    "    on_change(None)  # Initial trigger\n",
    "\n",
    "def plot_climate_heatmap(Y_train, climate_var='pr'):\n",
    "    \"\"\"\n",
    "    Creates an interactive heatmap for visualizing climate variables over time.\n",
    "\n",
    "    Parameters:\n",
    "    - Y_train (xarray.Dataset): Dataset containing climate variables.\n",
    "    - climate_var (str): Variable to visualize (default: 'pr' for precipitation).\n",
    "    \"\"\"\n",
    "    climate_data = Y_train[0][climate_var]  # Extract data for the first simulation\n",
    "\n",
    "    def plot_data(time_step=0):\n",
    "        fig = px.imshow(\n",
    "            climate_data.isel(time=time_step).values,\n",
    "            color_continuous_scale='viridis',\n",
    "            labels={'x': \"Longitude\", 'y': \"Latitude\"},\n",
    "            title=f\"{climate_var.upper()} at Time Step {time_step}\"\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "    return widgets.interactive(plot_data, time_step=(0, climate_data.sizes['time'] - 1, 1))\n",
    "\n",
    "\n",
    "def plot_climate_timeseries(Y_train, climate_var='pr', latitude=50.0, longitude=-120.0):\n",
    "    \"\"\"\n",
    "    Creates an interactive time series plot for a specific location.\n",
    "\n",
    "    Parameters:\n",
    "    - Y_train (xarray.Dataset): Climate dataset.\n",
    "    - climate_var (str): Climate variable to visualize.\n",
    "    - latitude (float): Latitude of the location.\n",
    "    - longitude (float): Longitude of the location.\n",
    "    \"\"\"\n",
    "    # Find the closest grid point\n",
    "    lat_idx = np.abs(Y_train[0]['latitude'] - latitude).argmin()\n",
    "    lon_idx = np.abs(Y_train[0]['longitude'] - longitude).argmin()\n",
    "\n",
    "    # Extract time series data for this location\n",
    "    climate_time_series = Y_train[0][climate_var][:, lat_idx, lon_idx]\n",
    "\n",
    "    # Create the interactive plot\n",
    "    fig = px.line(\n",
    "        x=Y_train[0]['time'],\n",
    "        y=climate_time_series,\n",
    "        labels={'x': \"Time\", 'y': f\"{climate_var.upper()}\"},\n",
    "        title=f\"{climate_var.upper()} Time Series at ({latitude}, {longitude})\"\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "def interactive_variable_selector(Y_train, plot_function):\n",
    "    \"\"\"\n",
    "    Creates an interactive dropdown menu to select a climate variable\n",
    "    and updates the visualization accordingly.\n",
    "\n",
    "    Parameters:\n",
    "    - Y_train (xarray.Dataset): Dataset containing climate variables.\n",
    "    - plot_function (function): Function to visualize the selected climate variable.\n",
    "    \"\"\"\n",
    "    variable_selector = widgets.Dropdown(\n",
    "        options=list(Y_train[0].data_vars.keys()),\n",
    "        description=\"Variable:\"\n",
    "    )\n",
    "\n",
    "    def update_variable(selected_var):\n",
    "        plot_function(Y_train, selected_var)  # Call the plotting function with the selected variable\n",
    "\n",
    "    return widgets.interactive(update_variable, selected_var=variable_selector)\n",
    "\n",
    "def compare_inputs_outputs(X_train_torch, Y_train_torch):\n",
    "    \"\"\"\n",
    "    Creates an interactive widget to compare input climate variables with\n",
    "    the predicted surface air temperature (TAS).\n",
    "\n",
    "    Parameters:\n",
    "    - X_train_torch (torch.Tensor): Input climate variables (samples, time, variables, height, width)\n",
    "    - Y_train_torch (torch.Tensor): Target temperature values (samples, time, height, width)\n",
    "    \"\"\"\n",
    "\n",
    "    def plot_sample(sample_idx, time_step):\n",
    "        \"\"\"\n",
    "        Helper function to plot climate variables and TAS for a given sample and time step.\n",
    "        \"\"\"\n",
    "        input_sample = X_train_torch[sample_idx, time_step].cpu().numpy()  # Shape: (4, 96, 144)\n",
    "        output_sample = Y_train_torch[sample_idx, 0].cpu().numpy()  # Shape: (96, 144)\n",
    "        variables = [\"CO₂\", \"CH₄\", \"SO₂\", \"Black Carbon\"]\n",
    "\n",
    "        fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "        for i in range(4):\n",
    "            im = axes[i].imshow(input_sample[i], cmap=\"coolwarm\", origin=\"lower\")\n",
    "            axes[i].set_title(variables[i])\n",
    "            fig.colorbar(im, ax=axes[i], shrink=0.6)\n",
    "\n",
    "        # Plot output TAS\n",
    "        im = axes[4].imshow(output_sample, cmap=\"coolwarm\", origin=\"lower\")\n",
    "        axes[4].set_title(\"Surface Air Temperature (TAS)\")\n",
    "        fig.colorbar(im, ax=axes[4], shrink=0.6)\n",
    "\n",
    "        plt.suptitle(f\"Comparison at Time Step {time_step}, Sample {sample_idx}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Interactive Widget\n",
    "    interact(plot_sample,\n",
    "             sample_idx=widgets.IntSlider(min=0, max=X_train_torch.shape[0]-1, step=1, value=0, description=\"Sample\"),\n",
    "             time_step=widgets.IntSlider(min=0, max=X_train_torch.shape[1]-1, step=1, value=0, description=\"Time Step\"))\n",
    "\n",
    "\n",
    "def animate_climate_variables(X_train_torch, sample_idx=0, scale_mode='auto'):\n",
    "    \"\"\"\n",
    "    Creates an interactive animation to visualize selected climate input variables\n",
    "    (CH₄, Black Carbon) over time, with optional log or linear scaling.\n",
    "    \"\"\"\n",
    "\n",
    "    # Clear previous figures and outputs\n",
    "    plt.close('all')\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # Extract input variables for the given sample\n",
    "    input_seq = X_train_torch[sample_idx].cpu().numpy()  # Shape: (time, variables, height, width)\n",
    "\n",
    "    # Only use CH₄ (1) and Black Carbon (3)\n",
    "    selected_indices = [1, 3]\n",
    "    variables = [\"CH₄\", \"Black Carbon\"]\n",
    "\n",
    "    # Prepare selected input\n",
    "    input_seq = input_seq[:, selected_indices, :, :]  # Now shape is (time, 2, H, W)\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4), constrained_layout=True)\n",
    "\n",
    "    # Precompute color scale limits or normalization\n",
    "    vmins, vmaxs, norms = [], [], []\n",
    "    for i in range(len(variables)):\n",
    "        data = input_seq[:, i, :, :]\n",
    "        data_flat = data.flatten()\n",
    "        all_positive = np.all(data_flat > 0)\n",
    "\n",
    "        if scale_mode == 'log' and all_positive:\n",
    "            norm = LogNorm(vmin=np.percentile(data_flat, 5), vmax=np.percentile(data_flat, 95))\n",
    "            norms.append(norm)\n",
    "            vmins.append(None)\n",
    "            vmaxs.append(None)\n",
    "        elif scale_mode == 'auto' and all_positive:\n",
    "            norm = LogNorm(vmin=np.percentile(data_flat, 5), vmax=np.percentile(data_flat, 95))\n",
    "            norms.append(norm)\n",
    "            vmins.append(None)\n",
    "            vmaxs.append(None)\n",
    "        else:\n",
    "            norm = None\n",
    "            norms.append(norm)\n",
    "            vmins.append(np.percentile(data_flat, 5))\n",
    "            vmaxs.append(np.percentile(data_flat, 95))\n",
    "\n",
    "    # Initialize image plots\n",
    "    ims = []\n",
    "    for i, ax in enumerate(axes):\n",
    "        if norms[i]:\n",
    "            im = ax.imshow(input_seq[0, i], cmap=\"coolwarm\", origin=\"lower\", norm=norms[i])\n",
    "        else:\n",
    "            im = ax.imshow(input_seq[0, i], cmap=\"coolwarm\", origin=\"lower\", vmin=vmins[i], vmax=vmaxs[i])\n",
    "        ax.set_title(f\"{variables[i]} (Year 1)\")\n",
    "        fig.colorbar(im, ax=ax, shrink=0.6)\n",
    "        ims.append(im)\n",
    "\n",
    "    # Animation update function\n",
    "    def update(frame):\n",
    "        for i, im in enumerate(ims):\n",
    "            im.set_data(input_seq[frame, i])\n",
    "            axes[i].set_title(f\"{variables[i]} (Year {frame+1})\")\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=input_seq.shape[0], interval=500, repeat=True)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display(HTML(ani.to_jshtml()))\n",
    "\n",
    "\n",
    "#  1. Live Loss & Validation Tracking\n",
    "def plot_loss():\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\", marker=\"o\")\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\", marker=\"s\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training & Validation Loss Over Time\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "#  2. Weight & Gradient Evolution (Histogram)\n",
    "def plot_weight_gradients(epoch):\n",
    "    if epoch < len(weights_history):\n",
    "        weights = np.concatenate([w.flatten() for w in weights_history[epoch]])\n",
    "        grads = np.concatenate([g.flatten() for g in grads_history[epoch]]) if grads_history[epoch] else None\n",
    "\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "        ax[0].hist(weights, bins=50, color=\"blue\", alpha=0.7)\n",
    "        ax[0].set_title(f\"Model Weights Distribution (Epoch {epoch+1})\")\n",
    "        ax[0].set_xlabel(\"Weight Value\")\n",
    "        ax[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "        if grads is not None:\n",
    "            ax[1].hist(grads, bins=50, color=\"red\", alpha=0.7)\n",
    "            ax[1].set_title(f\"Gradient Distribution (Epoch {epoch+1})\")\n",
    "            ax[1].set_xlabel(\"Gradient Value\")\n",
    "            ax[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "#  3. Sample Predictions Over Time (Slider)\n",
    "def plot_predictions1(epoch):\n",
    "    cnn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_input = X_train_torch[:10].to(next(cnn_model.parameters()).device)\n",
    "        Y_pred = cnn_model(X_input).cpu().numpy()\n",
    "\n",
    "        #Y_pred = cnn_model(X_train_torch[:10]).cpu().numpy()\n",
    "        Y_true = Y_train_torch[:10].cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(Y_true[epoch].flatten(), label=\"Ground Truth\", marker=\"o\")\n",
    "    plt.plot(Y_pred[epoch].flatten(), label=\"Predicted\", linestyle=\"dashed\", marker=\"x\")\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Climate Variable\")\n",
    "    plt.title(f\"Predictions vs. Ground Truth (Sample {epoch+1})\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def plot_predictions(y_true, y_pred, title):\n",
    "    \"\"\"Plots predicted vs. actual values as spatial maps.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    vmin, vmax = np.percentile(y_true, [5, 95])\n",
    "\n",
    "    axes[0].imshow(y_true.squeeze(), cmap='coolwarm', vmin=vmin, vmax=vmax)\n",
    "    axes[0].set_title('Ground Truth')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(y_pred.squeeze(), cmap='coolwarm', vmin=vmin, vmax=vmax)\n",
    "    axes[1].set_title('Prediction')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#for widgets\n",
    "def compare_inputs_outputs(sample_idx, time_step):\n",
    "    \"\"\"\n",
    "    Compare selected input climate variables (CH₄, Black Carbon) with predicted temperature change (TAS).\n",
    "    \"\"\"\n",
    "    input_sample = X_train_torch[sample_idx, time_step].cpu().numpy()  # (4, 96, 144)\n",
    "    output_sample = Y_train_torch[sample_idx, 0].cpu().numpy()  # (96, 144)\n",
    "\n",
    "    # Only include CH₄ (index 1) and Black Carbon (index 3)\n",
    "    selected_indices = [1, 3]\n",
    "    variables = [\"CH₄\", \"Black Carbon\"]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        im = axes[i].imshow(input_sample[idx], cmap=\"coolwarm\", origin=\"lower\")\n",
    "        axes[i].set_title(variables[i])\n",
    "        fig.colorbar(im, ax=axes[i], shrink=0.6)\n",
    "\n",
    "    # Plot output TAS\n",
    "    im = axes[2].imshow(output_sample, cmap=\"coolwarm\", origin=\"lower\")\n",
    "    axes[2].set_title(\"Surface Air Temperature (TAS)\")\n",
    "    fig.colorbar(im, ax=axes[2], shrink=0.6)\n",
    "\n",
    "    plt.suptitle(f\"Comparison at Time Step {time_step}, Sample {sample_idx}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qK_rGiNeZCw3",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set random seed, when using `pytorch` {\"run\":\"auto\",\"display-mode\":\"form\"}\n",
    "\n",
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "# for DL its critical to set the random seed so that students can have a\n",
    "# baseline to compare their results to expected results.\n",
    "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)\n",
    "\n",
    "\n",
    "set_seed(seed=2021, seed_torch=False)  # change 2021 with any number you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jKbb6F5eZCw3",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU). Execute `set_device()`\n",
    "# especially if torch modules used.\n",
    "\n",
    "# inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"GPU is not enabled in this notebook. But it will help make training faster if GPU is enabled. \\n\"\n",
    "          \"If you want to enable it, in the menu under `Runtime` -> \\n\"\n",
    "          \"`Hardware accelerator.` and select `GPU` from the dropdown menu\")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook. \\n\"\n",
    "          \"If you want to disable it, in the menu under `Runtime` -> \\n\"\n",
    "          \"`Hardware accelerator.` and select `None` from the dropdown menu\")\n",
    "\n",
    "  return device\n",
    "device= set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Pt0DOB8Ut_iA",
   "metadata": {
    "execution": {}
   },
   "source": [
    "> **Note:**  \n",
    "> GPU acceleration is optional for this tutorial. All code can be executed on CPUs, though some steps (especially model training) may take longer. Please allow additional time when running on CPU-only environments and meanwhile go through the remaining tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G9bZ6Qt-bbS7",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Deep Learning Techniques\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'XYmhvHVUbJE'), ('Bilibili', 'BV1ir7dzCEVA')]\n",
    "tab_contents = display_videos(video_ids, W=730, H=410)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8o047eEWb2cx",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Deep_Learning_Techniques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wcyhkzUqcA5c",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import IFrame\n",
    "\n",
    "link_id = \"abem5\"\n",
    "\n",
    "print(f\"If you want to download the slides: https://osf.io/download/{link_id}/\")\n",
    "IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/{link_id}/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dOmCfLncA-A",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Deep_Learning_Techniques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vH8FnuriZCw3",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 1. Transitioning to Deep Learning with PyTorch: From Machine Learning to Deep Learning in Climate Data Analysis\n",
    "\n",
    "## Section 1.1 Why Move from Machine Learning to Deep Learning?  \n",
    "\n",
    "In our previous tutorials, we used **Machine Learning (ML)** models, such as Random Forests and Gradient Boosting Machines, to analyze climate data. These models are effective when working with **structured, tabular datasets** but come with limitations:  \n",
    "\n",
    "1️⃣ **Manual Feature Engineering:** ML models require carefully **selected and engineered features**, which may not fully capture **hidden patterns** in climate data.  \n",
    "2️⃣ **Limited Spatial & Temporal Awareness:** Climate data is highly **spatiotemporal**, meaning relationships exist across **both space and time**—something ML models struggle to capture.  \n",
    "3️⃣ **Scalability Issues:** ML techniques work well on **small to medium-sized datasets**, but **struggle with high-dimensional and large-scale climate datasets**.\n",
    "\n",
    "🔹 **Deep Learning (DL)**, on the other hand, is designed to overcome these challenges by **automatically learning features** from raw, unstructured climate data. Unlike ML, DL models can handle large datasets, capture complex dependencies, and extract meaningful insights without the need for manual feature engineering.\n",
    "\n",
    "| 🔍 **Aspect**             | ⚡ **Machine Learning** (Previous Tutorials) | 🚀 **Deep Learning** (This Tutorial) |\n",
    "|-------------------------|---------------------------------|--------------------------------|\n",
    "| **Input Data**         | Structured tabular format      | Raw climate data (`NetCDF`)   |\n",
    "| **Feature Engineering** | Manual selection required      | Automatic feature extraction  |\n",
    "| **Spatial Awareness**   | Limited or none               | Captures spatial dependencies |\n",
    "| **Temporal Awareness**  | Limited                        | Captures long-term patterns   |\n",
    "| **Scalability**         | Suitable for small datasets   | Efficient for large datasets  |\n",
    "\n",
    "\n",
    "### Section 1.1.1 **ML Input vs. DL Input: What Changes?**\n",
    "\n",
    "Machine Learning in Previous Tutorials\n",
    "- **Input:** Climate variables from **2015** and projected emissions (2015–2050)  \n",
    "- **Output:** Predicted **2050 temperature anomaly**  \n",
    "- **Data Format:** Tabular representation with **location-scenario pairs**  \n",
    "- **Limitation:** Spatial and temporal dependencies were not explicitly preserved  \n",
    "\n",
    "While ML models performed well, they lacked the ability to capture complex **spatiotemporal relationships** present in climate data.\n",
    "\n",
    "Deep Learning for Spatiotemporal Data\n",
    "Deep learning enables us to work with **high-dimensional climate data** while maintaining its **spatial and temporal structure**. Instead of using tabular data, we now process climate data in its **original NetCDF format**, which includes:  \n",
    "\n",
    "- **Variables:** CO₂, CH₄, SO₂, BC  \n",
    "- **Dimensions:** `(time, latitude, longitude)`  \n",
    "- **Input:** Entire climate maps over time  \n",
    "- **Output:** Future climate projections at a grid level  \n",
    "\n",
    "**Why This Transition?**  \n",
    "\n",
    "| 🚀 **Advantage** | 🔍 **Benefit in Climate Modeling** |\n",
    "|-----------------|--------------------------------|\n",
    "| **Retains Spatial Structure** | Climate data is naturally spatial—DL can learn patterns across regions |\n",
    "| **Captures Temporal Trends** | Climate events are time-dependent—DL can model long-term patterns |\n",
    "| **Works with Raw Data** | No need for manual feature engineering—model extracts features directly |\n",
    "| **Uses CNNs & LSTMs** | Specialized layers handle both spatial (CNNs) and temporal (LSTMs) relationships |\n",
    "\n",
    "---\n",
    "<details>\n",
    "<summary> <font color='lightGreen'> Section 1.2 What is Deep Learning?  </font></summary>\n",
    "## Section 1.2 What is Deep Learning?  \n",
    "\n",
    "**Deep Learning (DL)** is a specialized branch of Machine Learning that uses **Artificial Neural Networks (ANNs)** to learn from data in a hierarchical manner. Instead of relying on **handcrafted features**, DL models extract patterns **directly from raw data** through multiple processing layers.\n",
    "\n",
    "**Key Components of Deep Learning**  \n",
    "\n",
    "🔹 **1. Neural Networks**  \n",
    "   - Deep learning models are composed of **neurons** (inspired by the human brain).  \n",
    "   - Neurons are organized into **layers**—each transforming the input data into meaningful representations.  \n",
    "   - **More layers = Deeper learning**, hence the term \"deep learning\".  \n",
    "\n",
    "🔹 **2. Training via Backpropagation**  \n",
    "   - The model learns by **adjusting weights** using a technique called **gradient descent**.  \n",
    "   - The error is **propagated backward** to refine the model iteratively.  \n",
    "\n",
    "🔹 **3. Deep Learning Architectures**  \n",
    "   - **Convolutional Neural Networks (CNNs)**: Ideal for processing spatial climate data (e.g., satellite images).  \n",
    "   - **Recurrent Neural Networks (RNNs) & LSTMs**: Designed for sequential data (e.g., temperature trends over time).  \n",
    "   - **Hybrid CNN-LSTM Models**: Capture **both spatial and temporal dependencies**—perfect for climate prediction.  \n",
    "\n",
    "---\n",
    "</details>\n",
    "<details>\n",
    "<summary> <font color='lightGreen'>Section 1.3 Why Deep Learning?  </font></summary>\n",
    "## Section 1.3 Why Deep Learning?\n",
    "*   **Automated Feature Extraction:** Deep learning models automatically discover complex relationships from the data, reducing reliance on manual feature engineering.\n",
    "*   **Spatiotemporal Modeling:** CNN-LSTMs can simultaneously analyze spatial patterns and temporal dependencies, surpassing the capabilities of simpler models.\n",
    "*   **Handles Complex Data:** Able to handle the high dimensional climate data more affectively than previous approaches.\n",
    "</details>\n",
    "<details>\n",
    "<summary> <font color='lightGreen'>Sectoin 1.4 Why PyTorch?  </font></summary>\n",
    "## Section 1.4 Why PyTorch?  \n",
    "\n",
    "To build our deep learning models, we will use **PyTorch**—one of the most widely used deep learning frameworks. PyTorch provides **flexibility, intuitive coding, and GPU acceleration**, making it an excellent choice for research and production applications.\n",
    "\n",
    "**Advantages of PyTorch**  \n",
    "✅ **Dynamic Computation Graphs:** Unlike TensorFlow, PyTorch builds computational graphs dynamically, making debugging easier.  \n",
    "✅ **Easy-to-Use API:** Simple, Pythonic syntax that integrates seamlessly with NumPy.  \n",
    "✅ **Efficient GPU Acceleration:** Allows rapid training on GPUs, making deep learning models highly scalable.  \n",
    "✅ **Robust Library Ecosystem:** Includes built-in modules for **automatic differentiation, optimization, and dataset handling**.\n",
    "\n",
    "**PyTorch Essentials for This Tutorial**  \n",
    "\n",
    "| 🔧 **PyTorch Module**  |  **Purpose** |\n",
    "|-------------------|--------------------------------------|\n",
    "| `torch.Tensor`  | Core data structure for PyTorch models |\n",
    "| `torch.nn`      | Provides layers like CNN, LSTM, etc. |\n",
    "| `torch.optim`   | Optimizers for training models |\n",
    "| `torch.autograd`| Automatic differentiation for backpropagation |\n",
    "| `torch.utils.data` | Handles datasets and dataloaders |\n",
    "\n",
    "💡 **In this tutorial, we will use PyTorch to implement a CNN-LSTM model for climate prediction, leveraging both spatial and temporal patterns in raw climate data.**\n",
    "\n",
    "---\n",
    "</details>\n",
    "<details>\n",
    "<summary> <font color='lightGreen'>Section 1.5 Critique of the Previous ML Tutorial: What Can Be Improved?  </font></summary>\n",
    "## Section 1.5 Critique of the Previous ML Tutorial: What Can Be Improved?\n",
    "\n",
    "While our **previous ML-based approach** was effective, it had some **limitations** that we aim to address with deep learning:  \n",
    "\n",
    "🔹 **Limited Generalization:** The ML model was trained on a **condensed dataset**, meaning it might not generalize well to **real-world, large-scale climate data**.  \n",
    "🔹 **Feature Engineering Dependency:** The performance of ML models heavily depends on **manual feature selection**, which is time-consuming and requires domain expertise.  \n",
    "🔹 **Inability to Capture Spatial/Temporal Dependencies:** Tree-based ML models treat input features as **independent variables**, ignoring crucial **spatial and temporal correlations** in climate data.  \n",
    "🔹 **Scalability Issues:** As climate datasets grow in size, **traditional ML methods struggle** to handle the increasing data complexity efficiently.\n",
    "\n",
    "By moving to **deep learning**, we address these shortcomings by:  \n",
    "✅ Using **raw, high-dimensional climate data** instead of condensed versions.  \n",
    "✅ Leveraging **CNNs and LSTMs** to automatically **learn patterns from spatial and temporal data**.  \n",
    "✅ Utilizing **GPU-accelerated PyTorch models** to efficiently handle large datasets.  \n",
    "</details>\n",
    "---\n",
    "🚀 Next, let's load and preprocess our climate dataset for deep learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_AVUh9EAcPsd",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Section_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q5t-SckmZCw4",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 2: ClimateBench Data Reloaded - Now in PyTorch!\n",
    "\n",
    "In this section, we transition from the `pandas` and `scikit-learn` world of Tutorials 1 and 2 to the tensor-centric universe of `PyTorch`.  We'll load a similar ClimateBench dataset, but prepare it for our `CNN-LSTM` architecture.\n",
    "\n",
    "As before, we need a set of tools. Note the key change: we bring in `torch` and its related modules:\n",
    "\n",
    "Note: For deateiled understanding of `Pytorch` and Deep leanring refer to Neuromacth Deep Learning Course's [W1D1Tutorial1](https://deeplearning.neuromatch.io/tutorials/W1D1_BasicsAndPytorch/student/W1D1_Tutorial1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-ocmORr7ZCw4",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "<details>\n",
    "<summary> <font color='lightGreen'>Working with Tensors: PyTorch's Core Data Structure</font></summary>\n",
    "\n",
    "\n",
    "PyTorch revolves around **tensors**, which are multi-dimensional arrays similar to NumPy arrays, but with the added benefit of GPU acceleration. Think of a tensor as the fundamental building block for representing data in neural networks.\n",
    "\n",
    "**Why Tensors?**\n",
    "\n",
    "*   **GPU Acceleration:** Enable lightning-fast computations for complex models.\n",
    "*   **Automatic Differentiation:** Seamlessly compute gradients for training.\n",
    "*   **Flexibility:** Represent various data types (floats, integers, etc.).\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary> <font color='lightGreen'>Essential Components for Climate-Informed Deep Learning</font></summary>\n",
    "\n",
    "\n",
    "*   **`torch.Tensor`**: The base data structure for representing climate variables (temperature, emissions, etc.).\n",
    "*   **`torch.nn`**: A module containing building blocks for defining our CNN-LSTM model architecture (convolutional layers, LSTM layers, etc.).\n",
    "*   **`torch.optim`**: Optimization algorithms (e.g., Adam) to train the model effectively.\n",
    "*   **`torch.utils.data.Dataset` & `torch.utils.data.DataLoader`**: Powerful tools for managing large climate datasets and efficiently feeding them into our model during training.\n",
    "---\n",
    "**Pytorch core Component Breakdown Table**\n",
    "\n",
    "| Component          | Symbol | Purpose                                  | Climate Application Example          |\n",
    "|--------------------|--------|------------------------------------------|---------------------------------------|\n",
    "| **Tensors**        | ⚡      | GPU-accelerated multidimensional arrays | Store 3D atmospheric data cubes       |\n",
    "| **nn.Module**      | 🧱      | Neural network building blocks          | Create CNN-LSTM hybrid architectures  |\n",
    "| **Optimizers**     | 🎯      | Parameter update strategies             | Adam for stable climate model training|\n",
    "| **DataLoaders**    | 📂      | Batch processing & shuffling            | Handle decades of climate observations|\n",
    "| **Loss Functions** | 📉      | Model performance quantification         | MSE for temperature prediction        |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"padding: 15px; background: #DFF9; border-radius: 5px; margin: 10px;\">\n",
    "<strong>🔄 Workflow Insight:</strong> Typical development pattern:\n",
    "1. Tensor Preparation → 2. Model Architecture → 3. Loss/Optimizer Setup → 4. Training Loop → 5. Validation\n",
    "</div>\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fPCT3GrTZCw4",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.1 The Shift to Spatiotemporal Data: ClimateBench in Native Format  \n",
    "\n",
    "In Tutorials 1 and 2, we trained machine learning models using a **simplified**, spatially-averaged dataset. While this approach was useful, it had limitations:  \n",
    "\n",
    "- **Loss of spatial information**, reducing the model's ability to capture regional climate variations  \n",
    "- **Limited temporal structure**, as time-series emissions were flattened into tabular form  \n",
    "\n",
    "Now, we transition to deep learning, unlocking the **full potential** of the ClimateBench dataset by preserving its original spatial and temporal structure.\n",
    "\n",
    "---\n",
    "<details>\n",
    "<summary> <font color='lightGreen'>Recap: What Was the Previous Data Format?</font></summary>\n",
    "\n",
    "### **What Was the Previous Data Format?**  \n",
    "\n",
    "Previously, we **averaged across spatial dimensions** and **flattened the emissions time series**, resulting in:  \n",
    "\n",
    "- **Shape:** `(3240, 152)`  \n",
    "  - **3240 rows** → location-scenario combinations  \n",
    "  - **152 columns** → 2015 climate variables + time-averaged emissions  \n",
    "\n",
    "This simplified dataset was easier to process with `scikit-learn`, but it **sacrificed critical spatial and temporal dependencies**.\n",
    "\n",
    "---\n",
    "</details>\n",
    "<details>\n",
    "<summary> <font color='lightGreen'>Recap: What Data  Are We going to Use Now?</font></summary>\n",
    "\n",
    "### **Recap: What Data  Are We going to Use Now?**  \n",
    "\n",
    "We now work with the original **NetCDF structure**, which explicitly retains all spatial and temporal information. This dataset will be structured as:  \n",
    "\n",
    "- **Input (`X`)** → `(766, 10, 96, 144, 4)`  \n",
    "  - **766 sequences** → extracted from all climate simulations  \n",
    "  - **10 time steps** → sliding window approach over years  \n",
    "  - **96 latitude points** → range: `-90° to 90°`  \n",
    "  - **144 longitude points** → range: `0° to 357.5°` (2.5° increments)  \n",
    "  - **4 climate variables** → CO₂, CH₄, SO₂, BC  \n",
    "\n",
    "- **Target (`Y`)** → `(766, 1, 96, 144)`  \n",
    "  - Single time-step temperature anomaly prediction  \n",
    "\n",
    "This structure allows deep learning models to **capture spatial dependencies** (across latitude and longitude) and **learn temporal trends** (using recurrent or convolutional layers).\n",
    "\n",
    "---\n",
    "</details>\n",
    "\n",
    "### **Overall**  \n",
    "\n",
    "In Tutorials 1 & 2, we worked with a **pre-processed**, tabular dataset optimized for `scikit-learn`.  \n",
    "Now, we will **directly load and process the raw ClimateBench dataset** using `xarray`. This ensures that our deep learning models can fully leverage the **spatiotemporal structure** of climate data.\n",
    "\n",
    "💡 **Next Step:** Let’s dive into the code and see how to load and preprocess this data! 🚀  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kzi33dVNZCw4",
   "metadata": {
    "execution": {}
   },
   "source": [
    "First, Define the path to the training data and then define the climate scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ca2fd7-c5a4-4afe-9b0b-102685728caf",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Section 2.1.1 Data Retrieval "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fd340e-6315-4e1d-a406-9ab8a759ccea",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Selection of Climate Scenarios for Model Training\n",
    "\n",
    "Choose one or more climate scenarios to train the model. More data can improve performance.  \n",
    "This cell can be re-executed to modify selections.  \n",
    "Available scenarios: `['ssp126', 'ssp370', 'ssp585', 'hist-GHG', 'hist-aer', 'historical']`\n",
    "\n",
    "> Tip: Better select all scenario for best possible model perfromance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b669c1b3-64e1-4272-9b46-1aed10b4df40",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Select and Download Climate Scenarios of your interest (Training Data)\n",
    "# @markdown Run this cell to enable the Climate Scenarios selector dropdown.\n",
    "\n",
    "setup_scenario_selector()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a598bc5-625b-4c5d-b035-ea90bbfef4c3",
   "metadata": {
    "execution": {}
   },
   "source": [
    "- Above cell downloads the required data to the temporary storage of your Colab session.\n",
    "- You can view the downloaded files under the folder icon 📁 on the left sidebar inside the sub-folder '*Data*' ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269e607c-bc28-46fa-b20a-ef9b7bce412f",
   "metadata": {
    "execution": {}
   },
   "source": [
    "- Above cell downloads the required data to the temporary storage of your Colab session.\n",
    "- You can view the downloaded files under the folder icon 📁 on the left sidebar inside the sub-folder '*Data*' ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c741155-b768-4269-bc9c-6018625d7650",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Select Input Variables\n",
    "# @markdown Run this cell to enable the input variables selector dropdown- slecet all for best performance\n",
    "\n",
    "# Call the function already defined under helper functions\n",
    "setup_climate_input_selector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1718d8-e4a5-41e3-b53e-cbb94b22502b",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "data_path = \"Data/train_val/\"  # Path to data in Colab RAM\n",
    "len_historical = 165  # Historical period length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_WRHRwUlZCw5",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.2 Loading & Processing The Full Raw ClimateBench Data  \n",
    "\n",
    "In this section, we **transition from pre-processed datasets to raw NetCDF files** while ensuring correct alignment and standardization for deep learning.  \n",
    "\n",
    "**What We Are Doing in next code cell**:  \n",
    "✔️ **Load historical & future climate data** using `xarray` from NetCDF files.  \n",
    "✔️ **Merge historical and scenario-specific simulations** to create a continuous time series.  \n",
    "✔️ **Standardize dimensions, variable names, and units** for consistency.  \n",
    "✔️ **Rescale precipitation data** from kg/m²/s to mm/day.  \n",
    "✔️ **Drop unnecessary variables** to optimize memory usage.  \n",
    "\n",
    "**New Functions & Methods to Look Out For:**  \n",
    "🔹 `xr.open_dataset()` → Loads single NetCDF files into an `xarray.Dataset`.  \n",
    "🔹 `xr.open_mfdataset()` → Efficiently loads and merges multiple NetCDF files.  \n",
    "🔹 `xr.concat()` → Concatenates datasets along a specified dimension.  \n",
    "🔹 `mean(dim='member')` → Computes the ensemble mean across climate model members.  \n",
    "🔹 `.transpose()` → Ensures correct ordering of dimensions (`time`, `latitude`, `longitude`).  \n",
    "\n",
    "This step **prepares the dataset for deep learning**, ensuring that temporal and spatial relationships are correctly preserved.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZAfP6CooZCw5",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Make sure you execute this cell to load the data in two variables `X_train` and `Y_train`\n",
    "\n",
    "# Initialize empty lists to store training input (X) and output (Y) data\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "# Iterate through each climate simulation in the 'simus' list\n",
    "for i, simu in enumerate(scenario_selected):\n",
    "    # Construct filenames for input and output NetCDF files\n",
    "    input_name = 'inputs_' + simu + '.nc'\n",
    "    output_name = 'outputs_' + simu + '.nc'\n",
    "\n",
    "    # Load input and output data using xarray\n",
    "    if 'hist' in simu:  # Check if the simulation is historical\n",
    "        # Open the input dataset for historical simulation\n",
    "        input_xr = xr.open_mfdataset(data_path + input_name)\n",
    "\n",
    "        # Open the output dataset and compute the mean across 'member' dimension\n",
    "        # 'member' refers to different realizations (ensemble members) of the climate model\n",
    "        output_xr = xr.open_mfdataset(data_path + output_name).mean(dim='member')\n",
    "\n",
    "    else:  # If it's a future scenario simulation\n",
    "        # Open historical input and scenario-specific input, merging them together\n",
    "        # 'open_mfdataset' allows handling multiple files efficiently\n",
    "        input_xr = xr.mfopen_mfdataset([data_path + 'inputs_historical.nc', data_path + input_name]).compute()\n",
    "\n",
    "        # Open and concatenate historical & scenario-specific output data along the 'time' dimension\n",
    "        # Compute the mean across ensemble members for consistency\n",
    "        output_xr = xr.concat([\n",
    "            xr.open_mfdataset(data_path + 'outputs_historical.nc').mean(dim='member'),\n",
    "            xr.open_mfdataset(data_path + output_name).mean(dim='member')\n",
    "        ], dim='time').compute()\n",
    "\n",
    "    # Standardizing variable names and units for consistency\n",
    "    output_xr = output_xr.assign({\n",
    "        \"pr\": output_xr.pr * 86400,  # Convert precipitation from kg/m²/s to mm/day\n",
    "        \"pr90\": output_xr.pr90 * 86400  # Convert 90th percentile precipitation similarly\n",
    "    }).rename({'lon': 'longitude', 'lat': 'latitude'})  # Rename dimensions for clarity\n",
    "\n",
    "    # Ensure the dataset follows the correct ordering of dimensions\n",
    "    output_xr = output_xr.transpose('time', 'latitude', 'longitude')\n",
    "    input_xr = input_xr.transpose('time', 'latitude', 'longitude')\n",
    "    # Drop unnecessary variables (like 'quantile') to save memory\n",
    "    output_xr = output_xr.drop(['quantile'])\n",
    "\n",
    "    # Print the dataset dimensions to verify correctness\n",
    "    #print(input_xr.dims, simu)\n",
    "\n",
    "\n",
    "    # Append processed input and output datasets to training lists\n",
    "    X_train.append(input_xr)\n",
    "    Y_train.append(output_xr)\n",
    "\n",
    "print(\"The data has been successfully loaded into `X_train` and `Y_train`!\")\n",
    "# Print the shape of the first element (xarray.Dataset) in the list\n",
    "print(\"The shape of the `X_train[0]` is: \", X_train[0].sizes)\n",
    "print(\"The shape of the `Y_train[0]` is: \", Y_train[0].sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "L1QhMDJ4ZCw5",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "<details>\n",
    "<summary> <font color='lightGreen'>Code explaination?  </font></summary>\n",
    "\n",
    "\n",
    "We have now successfully **loaded and preprocessed the ClimateBench dataset** in its native NetCDF format. Here's a breakdown:  \n",
    "\n",
    "📌 **Historical vs. Future Simulations** →  \n",
    "- If the dataset is historical (`hist` in `simu`), we load it directly.  \n",
    "- If it's a future scenario, we merge it with historical data to maintain continuity.  \n",
    "\n",
    "📌 **Preprocessing Steps Applied** →  \n",
    "- **Ensured temporal consistency** using concatenation (`xr.concat()`).  \n",
    "- **Standardized spatial dimensions** (`longitude`, `latitude`).  \n",
    "- **Converted precipitation units** for better interpretability.  \n",
    "- **Dropped unnecessary variables** to optimize storage and speed.  \n",
    "\n",
    "\n",
    "\n",
    "**💡 Key Takeaways**  \n",
    "✅ **Why This Matters?** Our model will now work with the full spatiotemporal structure instead of a condensed dataset, capturing richer patterns.  \n",
    "✅ **What’s Next?** Now that the data is correctly formatted, we move to **data transformation and model input preparation**! 🚀  \n",
    "\n",
    "**Next:- Preparing for Data Normalization**\n",
    "Before feeding our climate data into a deep learning model, we need to ensure that all features are on a comparable scale. Different climate variables have different units and magnitudes (e.g., CO₂ in ppm vs. precipitation in mm/day), which can negatively impact model performance.  \n",
    "\n",
    "In the next section, we'll explore **data normalization**, understand why it is essential, and implement a standardization technique to transform our dataset for optimal learning.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j5xNpiQbZCw5",
   "metadata": {
    "execution": {}
   },
   "source": [
    "So we have just loaded the datatset in two variables `X_train` and `Y_train`\n",
    "\n",
    "Let' Check the content inside both the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4KBKWDFaZCw5",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NHQO55-yZCw5",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lvIvQxSsZCw6",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.3 Visualization of the Data\n",
    "\n",
    "1️⃣ **Interactive Climate Data Heatmap**   \n",
    "- Display a heatmap of climate variables (e.g., temperature, precipitation) at different time steps.  \n",
    "- Use a slider to navigate through time dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m9Q6dWsLlL_X",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Run this cell to activate Interactive Climate Data Heatmap {\"run\":\"auto\",\"vertical-output\":true,\"display-mode\":\"form\"}\n",
    "\n",
    "# Select a variable to visualize (e.g., 'pr' for precipitation)\n",
    "climate_var = 'pr'  # Change to any variable you want\n",
    "scale_mode = 'log'  # or 'linear'\n",
    "\n",
    "# Extract data from the first simulation (Modify based on your needs)\n",
    "climate_data = Y_train[0][climate_var]\n",
    "\n",
    "# Define an interactive function to update the heatmap\n",
    "def plot_climate_data(time_step=0):\n",
    "    data = climate_data.isel(time=time_step).values\n",
    "\n",
    "    if scale_mode == 'log':\n",
    "        data_clipped = np.clip(data, a_min=1e-6, a_max=None)\n",
    "        data = np.log10(data_clipped)\n",
    "        colorbar_title = f\"log10({climate_var.upper()})\"\n",
    "    else:\n",
    "        colorbar_title = climate_var.upper()\n",
    "\n",
    "    fig = px.imshow(\n",
    "        data,\n",
    "        color_continuous_scale='viridis',\n",
    "        labels={'x': \"Longitude\", 'y': \"Latitude\"},\n",
    "        title=f\"{climate_var.upper()} at Time Step {time_step}\",\n",
    "    )\n",
    "    fig.update_coloraxes(colorbar_title=colorbar_title)\n",
    "    fig.show()\n",
    "# Create an interactive slider for time navigation\n",
    "widgets.interactive(plot_climate_data, time_step=(0, climate_data.sizes['time'] - 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cxs-TyzbZCw6",
   "metadata": {
    "execution": {}
   },
   "source": [
    "2️⃣ **Interactive Time Series for a Specific Location**  \n",
    "Allow users to select a location (lat, lon) and see how a climate variable changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ep22U2vOZCw6",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Run this cell to activate widget\n",
    "plot_climate_timeseries(Y_train, climate_var='pr', latitude=30.0, longitude=-100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZqAx3683ZCw6",
   "metadata": {
    "execution": {}
   },
   "source": [
    "3️⃣ ***Widget for Selecting Different Climate Variables***  \n",
    "Use the dropdown widget to let yourself explore different variables dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x1tCNfhWZCw6",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Run this cell to activate Interactive widget {\"run\":\"auto\",\"vertical-output\":true,\"display-mode\":\"form\"}\n",
    "\n",
    "# Create a dropdown menu to select a climate variable\n",
    "variable_selector = widgets.Dropdown(\n",
    "    options=list(Y_train[0].data_vars.keys()),\n",
    "    description=\"Variable:\"\n",
    ")\n",
    "\n",
    "# Define a function to update the visualization based on selection\n",
    "def update_variable(selected_var):\n",
    "    global climate_var\n",
    "    climate_var = selected_var\n",
    "    plot_climate_data(0)\n",
    "\n",
    "# Link the dropdown to the function\n",
    "widgets.interactive(update_variable, selected_var=variable_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0IwPUX6Gccj8",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Section_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Js7OcYAAZCw6",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 3: Data Normalization\n",
    "\n",
    "Data normalization is a fundamental preprocessing step in deep learning, ensuring that input features are on a similar scale. This improves model training stability and overall performance.\n",
    "\n",
    "\n",
    "**Why is Data Normalization Essential?**  \n",
    "In deep learning, input features can have vastly different scales. For instance, CO₂ levels are measured in **ppm**, while SO₂ concentrations are in **ppb**. Without normalization, models struggle to learn effectively due to imbalanced feature magnitudes.  \n",
    "\n",
    "<details>\n",
    "<summary> <font color='lightGreen'>Section 3.1 Benefits of Normalization</font></summary>\n",
    "\n",
    "## Section 3.1 Benefits of Normalization  \n",
    "\n",
    "✅ **Faster & More Stable Training**  \n",
    "- Gradient-based optimizers converge efficiently when features are on a similar scale.  \n",
    "- Avoids issues like vanishing/exploding gradients.  \n",
    "\n",
    "✅ **Improved Model Generalization**  \n",
    "- Prevents dominance of high-magnitude features over low-magnitude ones.  \n",
    "- Reduces model sensitivity to varying data distributions.  \n",
    "\n",
    "✅ **Numerical Stability**  \n",
    "- Helps prevent extreme weight updates during backpropagation.  \n",
    "- Standardized inputs maintain consistent learning dynamics across different datasets.  \n",
    "\n",
    "---\n",
    "</details>\n",
    "<details>\n",
    "<summary> <font color='lightGreen'>Section 3.2 Standardization: The Chosen Normalization Technique</font></summary>\n",
    "\n",
    "## Section 3.2 Standardization: The Chosen Normalization Technique\n",
    "\n",
    "We apply **Z-score normalization**, a widely used method in deep learning:  \n",
    "$$\n",
    "X_{\\text{normalized}} = \\frac{X - \\mu}{\\sigma}\n",
    "$$\n",
    "Where:  \n",
    "- \\(X\\) = Original feature value  \n",
    "- \\(\\mu\\) = Mean of the feature  \n",
    "- \\(\\sigma\\) = Standard deviation of the feature  \n",
    "\n",
    "**Why Z-score Normalization?**  \n",
    "✔ Ensures a **mean of 0** and a **standard deviation of 1**, making features comparable.  \n",
    "✔ Retains outlier sensitivity while improving learning efficiency.  \n",
    "\n",
    "---\n",
    "</details>\n",
    "\n",
    "## Section 3.3 Implementing Normalization\n",
    "\n",
    "**What We’ll Do Next:**  \n",
    "🔹 Define functions for **normalizing and unnormalizing** the data.  \n",
    "🔹 Compute **mean and standard deviation** for key climate variables (**CO₂, CH₄, SO₂, BC**).  \n",
    "🔹 Apply **Z-score transformation** to the input dataset.  \n",
    "\n",
    "This ensures our data is preprocessed optimally for deep learning. Let’s proceed! 🚀  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ynGn2aodZCw7",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# **3. Data Normalization**\n",
    "\n",
    "# Utility function to normalize data using Z-score normalization\n",
    "def normalize(data, var, meanstd_dict):\n",
    "    \"\"\"\n",
    "    Applies standardization (Z-score normalization) to the given variable.\n",
    "    Formula: (X - mean) / std\n",
    "    - data: The input array for the variable.\n",
    "    - var: Name of the variable being normalized.\n",
    "    - meanstd_dict: Dictionary containing mean and standard deviation for each variable.\n",
    "\n",
    "    Returns:\n",
    "        Normalized data with mean 0 and standard deviation 1.\n",
    "    \"\"\"\n",
    "    mean = meanstd_dict[var][0]  # Extract mean for the variable\n",
    "    std = meanstd_dict[var][1]   # Extract standard deviation for the variable\n",
    "    return (data - mean) / std    # Apply normalization\n",
    "\n",
    "\n",
    "# Utility function to revert normalized data back to its original scale\n",
    "def unnormalize(data, var, meanstd_dict):\n",
    "    \"\"\"\n",
    "    Converts standardized data back to its original scale.\n",
    "    Formula: X = (X_normalized * std) + mean\n",
    "    - data: The normalized array.\n",
    "    - var: Name of the variable to be unnormalized.\n",
    "    - meanstd_dict: Dictionary containing mean and standard deviation for each variable.\n",
    "\n",
    "    Returns:\n",
    "        Unnormalized data in its original scale.\n",
    "    \"\"\"\n",
    "    mean = meanstd_dict[var][0]  # Extract mean for the variable\n",
    "    std = meanstd_dict[var][1]   # Extract standard deviation for the variable\n",
    "    return data * std + mean      # Apply inverse transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aAE4_Z5ZCw7",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# **Step 1: Compute mean and standard deviation for each variable across the dataset**\n",
    "meanstd_inputs = {}  # Dictionary to store computed mean and standard deviation\n",
    "\n",
    "\n",
    "#selected_climate_input_vars = ['CO2', 'CH4', 'SO2', 'BC'] # uncomment this for better result\n",
    "selected_simulations_ids = range(0, len(X_train))\n",
    "for var in selected_climate_input_vars:  # Iterate over selected climate variables;selected_climate_input_vars came from the hiddeb cell upon the dropdown slection\n",
    "    # Concatenate relevant data samples across historical and future simulations\n",
    "    array = np.concatenate(\n",
    "        [X_train[i][var].data for i in selected_simulations_ids] +  # Directly use data from selected simulations\n",
    "        [X_train[i][var].sel(time=slice(len_historical, None)).data for i in selected_simulations_ids]  # Use post-historical data\n",
    "    )\n",
    "\n",
    "    print((array.mean(), array.std()))  # Display computed mean and standard deviation\n",
    "    meanstd_inputs[var] = (array.mean(), array.std())  # Store computed values in dictionary\n",
    "\n",
    "\n",
    "# **Step 2: Normalize the input dataset using computed statistics**\n",
    "X_train_norm = []  # List to store normalized training data\n",
    "\n",
    "for i, train_xr in enumerate(X_train):  # Iterate over each training sample\n",
    "    for var in selected_climate_input_vars:  # Process each selected climate variable\n",
    "        var_dims = train_xr[var].dims  # Retrieve variable's dimension structure (e.g., time, lat, lon)\n",
    "\n",
    "        # Apply normalization and assign the transformed values back to the dataset\n",
    "        train_xr = train_xr.assign({var: (var_dims, normalize(train_xr[var].data, var, meanstd_inputs))})\n",
    "\n",
    "    X_train_norm.append(train_xr)  # Append the normalized dataset to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vI5C6mu0c80c",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Section_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UzZ6HlgFZCw7",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 4: Reshaping Data for the CNN-LSTM Model\n",
    "\n",
    "Deep learning models require input data in a structured format to efficiently learn temporal dependencies.  \n",
    "Here we will **reshape our time-series data** using a **sliding window approach**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ldlD7S70Uesi",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "## Section 4.1 The Sliding Window Approach\n",
    "\n",
    "The **CNN-LSTM model** captures temporal dependencies by processing overlapping sequences of data.  \n",
    "Each sequence consists of **10 consecutive years** (as defined by `slider = 10`).  \n",
    "The model then predicts the next time step based on the previous sequence.\n",
    "\n",
    "**Example:** Understanding the Sliding Window Mechanism\n",
    "\n",
    "Let’s say we have a time series dataset:\n",
    "\n",
    "| Year  | Feature Value |\n",
    "|--------|--------------|\n",
    "| 2000  | 2.5          |\n",
    "| 2001  | 2.8          |\n",
    "| 2002  | 3.1          |\n",
    "| 2003  | 3.4          |\n",
    "| 2004  | 3.7          |\n",
    "| 2005  | 4.0          |\n",
    "\n",
    "With a **window size of 3**, the sequences formed are:\n",
    "\n",
    "| Input Sequence      | Target Value |\n",
    "|---------------------|-------------|\n",
    "| [2.5, 2.8, 3.1]    | 3.4         |\n",
    "| [2.8, 3.1, 3.4]    | 3.7         |\n",
    "| [3.1, 3.4, 3.7]    | 4.0         |\n",
    "\n",
    "This transformation allows the model to learn from past values and predict future trends.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G20dOxtwUgTG",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "## Section 4.2 Implementing the Sliding Window Transformation\n",
    "\n",
    "Below, we define two key functions:\n",
    "- **`input_for_training`** → Reshapes the input dataset into overlapping sequences.  \n",
    "- **`output_for_training`** → Extracts the corresponding target values.\n",
    "\n",
    "Now, let's implement the reshaping process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k1Ay81S7UnSS",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# **4. Reshape Data to Feed into the Model**\n",
    "slider = 10  # Defines the sliding window size (10 years of past data used for prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v5zBq6vvZCw8",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Function to reshape input data for training\n",
    "def input_for_training(X_train_xr, skip_historical=False, len_historical=None):\n",
    "    \"\"\"\n",
    "    Reshapes input climate data into a time-series format suitable for CNN-LSTM.\n",
    "\n",
    "    - Converts xarray dataset to NumPy and rearranges axes to (time, lat, lon, variable).\n",
    "    - Uses a sliding window approach to create overlapping sequences.\n",
    "    - Can skip historical data based on the `skip_historical` flag.\n",
    "\n",
    "    Parameters:\n",
    "        X_train_xr: xarray dataset containing input climate variables.\n",
    "        skip_historical: Whether to exclude historical data and start from the first future scenario point.\n",
    "        len_historical: Number of historical time steps, required if `skip_historical=True`.\n",
    "\n",
    "    Returns:\n",
    "        4D NumPy array of shape (num_samples, slider, lat, lon, variables)\n",
    "    \"\"\"\n",
    "    X_train_np = X_train_xr.to_array().transpose('time', 'latitude', 'longitude', 'variable').data  # Convert xarray to NumPy\n",
    "\n",
    "    time_length = X_train_np.shape[0]  # Total time steps in dataset\n",
    "\n",
    "    if skip_historical:\n",
    "        # Create sequences starting from the first non-historical point\n",
    "        X_train_to_return = np.array(\n",
    "            [X_train_np[i:i + slider] for i in range(len_historical - slider + 1, time_length - slider + 1)]\n",
    "        )\n",
    "    else:\n",
    "        # Create sequences spanning both historical and future scenario data\n",
    "        X_train_to_return = np.array(\n",
    "            [X_train_np[i:i + slider] for i in range(0, time_length - slider + 1)]\n",
    "        )\n",
    "\n",
    "    return X_train_to_return  # Return reshaped input sequences\n",
    "\n",
    "\n",
    "# Function to reshape output data (target variable) for training\n",
    "def output_for_training(Y_train_xr, var, skip_historical=False, len_historical=None):\n",
    "    \"\"\"\n",
    "    Reshapes the target variable into a time-series format, aligned with input sequences.\n",
    "\n",
    "    - Extracts the target variable as a NumPy array.\n",
    "    - Uses the sliding window approach to create target values corresponding to each input sequence.\n",
    "    - Can skip historical data based on the `skip_historical` flag.\n",
    "\n",
    "    Parameters:\n",
    "        Y_train_xr: xarray dataset containing target climate variable.\n",
    "        var: The specific variable to predict (e.g., temperature, CO₂ levels).\n",
    "        skip_historical: Whether to exclude historical data and start from the first future scenario point.\n",
    "        len_historical: Number of historical time steps, required if `skip_historical=True`.\n",
    "\n",
    "    Returns:\n",
    "        2D NumPy array of shape (num_samples, 1), where each row is the target value for a sequence.\n",
    "    \"\"\"\n",
    "    Y_train_np = Y_train_xr[var].data  # Extract the target variable\n",
    "\n",
    "    time_length = Y_train_np.shape[0]  # Total time steps in dataset\n",
    "\n",
    "    if skip_historical:\n",
    "        # Extract the last time step in each sequence as the target value, starting from the future scenario\n",
    "        Y_train_to_return = np.array(\n",
    "            [[Y_train_np[i + slider - 1]] for i in range(len_historical - slider + 1, time_length - slider + 1)]\n",
    "        )\n",
    "    else:\n",
    "        # Extract the last time step in each sequence as the target value, covering historical + future data\n",
    "        Y_train_to_return = np.array(\n",
    "            [[Y_train_np[i + slider - 1]] for i in range(0, time_length - slider + 1)]\n",
    "        )\n",
    "\n",
    "    return Y_train_to_return  # Return reshaped target values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HsEuYfGmZCw8",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "<details>\n",
    "<summary> <font color='lightGreen'>Expected Output: Why Is This Important?</font></summary>\n",
    "\n",
    "**Expected Output: Why Is This Important?**\n",
    "After reshaping, the dataset will have:\n",
    "- A **4D input structure** → `(num_samples, 10, lat, lon, features)`\n",
    "- A **2D target structure** → `(num_samples, 1)`\n",
    "\n",
    "This ensures:\n",
    "✅ The model **learns from past trends** effectively.  \n",
    "✅ It **preserves spatial and temporal dependencies** within the dataset.  \n",
    "✅ The CNN-LSTM network can process **structured time-series data** for accurate climate predictions.\n",
    "</details>\n",
    "<details>\n",
    "<summary> <font color='lightGreen'>Key Takeaways</font></summary>\n",
    "\n",
    "**4.4 Key Takeaways**\n",
    "🔹 Without proper formatting, deep learning models **cannot learn time dependencies effectively**.  \n",
    "🔹 The **sliding window technique** ensures each sequence captures meaningful temporal patterns.  \n",
    "🔹 Our data is now **ready for training!** 🎯  \n",
    "\n",
    "🚀 Next Steps: Data Preparation  \n",
    "To ensure stable training, it's essential to structure the dataset correctly. This involves careful **batching** to maintain the expected input shape, **normalization** to prevent unstable gradients, and **reshaping** to align outputs properly for meaningful predictions.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DwqH78qVc_3-",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Section_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "B7uMkGyQZCw8",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 5: Preparing the Training Data\n",
    "<details>\n",
    "<summary> <font color='lightGreen'>Why Does Data Preparation Matter in Deep Learning?</font></summary>\n",
    "###  Why Does Data Preparation Matter in Deep Learning?\n",
    "\n",
    "Unlike traditional machine learning, where data is often structured in simple tabular form, deep learning models—especially those handling spatiotemporal data—require a more complex input structure. ClimateBench provides **high-dimensional climate data** with both spatial (latitude, longitude) and temporal (years) components. To effectively train our CNN-LSTM model, we must **carefully organize and format** this data.\n",
    "</details>\n",
    "<details>\n",
    "<summary> <font color='lightGreen'>How Does PyTorch Handle This?</font></summary>\n",
    "How Does PyTorch Handle This?\n",
    "In PyTorch, deep learning models work with **tensors**, which are multi-dimensional arrays optimized for GPU computations. However, for models like CNN-LSTM, **the order and structure of these tensors are critical**. The model expects data in a specific format:  \n",
    "**(batch_size, timesteps, channels, height, width)**.  \n",
    "This structure ensures that:\n",
    "- CNN layers process spatial patterns independently for each time step.\n",
    "- LSTM layers capture long-term dependencies over time.\n",
    "- The model receives inputs in a way that aligns with its internal computations.\n",
    "</details>\n",
    "</details>\n",
    "\n",
    "What Needs to Be Done?\n",
    "\n",
    "To get our dataset ready for training, we must:  \n",
    "- 1️⃣ **Concatenate data across multiple climate scenarios** → Since our dataset comes from different emissions scenarios, we need to merge them into a single dataset.  \n",
    "- 2️⃣ **Convert NumPy arrays to PyTorch tensors** → This allows efficient tensor operations and enables GPU acceleration.  \n",
    "- 3️⃣ **Rearrange dimensions** → PyTorch tensors follow a specific order, so we use `permute()` to align the input format with what our CNN-LSTM expects.\n",
    "\n",
    "By preparing our data correctly, we ensure smooth training and avoid shape mismatches that could cause errors.\n",
    "</details>\n",
    "\n",
    "🔜 Next: Implementing these preprocessing steps in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mG_iZYI_ZCw8",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Section 5: Preparing Training Data for CNN-LSTM Model\n",
    "\n",
    "# Define the target climate variable (surface air temperature)\n",
    "var_to_predict = 'tas'\n",
    "\n",
    "if len(scenario_selected) == 1 and 'hist' in scenario_selected[0]:\n",
    "    skip_flags = [False]  # don't skip\n",
    "else:\n",
    "    skip_flags = [(i < 2) for i in range(len(scenario_selected))]\n",
    "\n",
    "# Step 1: Concatenate input data across all climate scenarios\n",
    "# - Climate data comes from multiple emission scenarios (scenario_selected list)\n",
    "# - Historical data is skipped for early simulations (i < 2)\n",
    "# - `len_historical` ensures proper alignment across different scenarios\n",
    "X_train_all = np.concatenate([\n",
    "    input_for_training(X_train_norm[i],\n",
    "                       skip_historical=skip_flags[i],\n",
    "                       len_historical=len_historical)\n",
    "    for i in range(len(scenario_selected))], axis=0)\n",
    "\n",
    "# Step 2: Concatenate output data (target variable: TAS)\n",
    "# - The same logic is applied to ensure consistent alignment\n",
    "Y_train_all = np.concatenate([\n",
    "    output_for_training(Y_train[i], var_to_predict,\n",
    "                        skip_historical=skip_flags[i],\n",
    "                        len_historical=len_historical)\n",
    "    for i in range(len(scenario_selected))], axis=0)\n",
    "\n",
    "# Verify dataset shapes before conversion\n",
    "print(\"Input shape:\", X_train_all.shape)  # for full.Expected: (726, 10, 96, 144, 4)\n",
    "print(\"Output shape:\", Y_train_all.shape)  # for full'Expected: (726, 1, 96, 144)\n",
    "\n",
    "# Step 3: Convert input dataset to PyTorch tensor & reorder dimensions\n",
    "# - `.permute(0, 1, 4, 2, 3)` rearranges axes to match (batch, timesteps, channels, height, width)\n",
    "X_train_torch = torch.tensor(X_train_all, dtype=torch.float32).permute(0, 1, 4, 2, 3)\n",
    "\n",
    "# Step 4: Convert output dataset to PyTorch tensor\n",
    "Y_train_torch = torch.tensor(Y_train_all, dtype=torch.float32)\n",
    "\n",
    "# Final shape verification after conversion\n",
    "print(\"PyTorch Input Shape:\", X_train_torch.shape)  # In case of all data selected; Expected: (726, 10, 4, 96, 144)\n",
    "print(\"PyTorch Output Shape:\", Y_train_torch.shape)  # in case of all data selected; Expected: (726, 1, 96, 144)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30-Kg26vZCw8",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The above code prepares climate data for training a CNN-LSTM model by structuring it into PyTorch tensors. First, it concatenates climate data from multiple emission scenarios to ensure a unified dataset. Next, it converts NumPy arrays to PyTorch tensors, enabling GPU acceleration. Finally, it rearranges tensor dimensions using .permute() to match the model’s expected format: (batch_size, timesteps, channels, height, width). These steps ensure the data is properly formatted for deep learning, preventing shape mismatches during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tcw0do7CZCw8",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "X_train_torch[0][0][0][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IuHd4HzwZCw9",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<details><summary> <font color='lightGreen'>Recap: Understnding ClimateBench Dataset Shape for Deep Learning  </font></summary>\n",
    "\n",
    "Recap: Understnding ClimateBench Dataset Shape for Deep Learning\n",
    "\n",
    "The **ClimateBench dataset** originates from advanced Earth System Model simulations (NorESM2) and captures spatiotemporal climate patterns across different emission scenarios.\n",
    "\n",
    " **Key Differences froM Understanding Traditional ML Approaches**\n",
    "- **ML Approaches (Tutorial 1 & 2):** Used globally averaged emission data.\n",
    "- **Deep Learning (This Tutorial):** Maintains full-resolution climate grids, preserving spatial variations.\n",
    "---\n",
    " **Dataset Structure**\n",
    "- **Input Variables:** CO₂, CH₄, SO₂, and Black Carbon.\n",
    "- **Target Variable:** Surface Air Temperature (TAS).\n",
    "- **Time Dimension:** Each input contains 10 years of sequential data.\n",
    "- **Spatial Grid:** 96 × 144 (latitude × longitude).\n",
    "---\n",
    "**5D Tensor Representation**\n",
    "Unlike simpler tabular ML datasets, our input is a **5D tensor**:\n",
    "- `(726, 10, 96, 144, 4)`, where:\n",
    "  - `726` → Sequences from multiple climate simulations.\n",
    "  - `10` → Temporal length (years per sequence).\n",
    "  - `96 × 144` → Spatial resolution.\n",
    "  - `4` → Climate input variables.\n",
    "\n",
    "This setup allows the model to learn **spatiotemporal climate dynamics**, capturing both spatial and temporal dependencies for robust predictions.\n",
    "\n",
    "Now that we have prepared the training data, we need to define the model, loss function, and optimizer.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seypdDgaZCw9",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 5.1: Interactive Climate Data Explorer  \n",
    "\n",
    "This widget allows you to explore climate input data dynamically. You can select a specific climate simulation (0–725) and adjust the time step (0–9) to see how the data evolves. Toggle between different climate variables—CO₂, CH₄, SO₂, and Black Carbon—to compare their spatial distributions on a 96 × 144 grid. The visualization updates instantly, providing an intuitive way to analyze spatiotemporal patterns without rerunning code. 🚀  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RPtDS1JRZCw9",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Make sure you execute this cell to enable the widget!\n",
    "\n",
    "# Enable the widget\n",
    "interact(compare_inputs_outputs,\n",
    "         sample_idx=widgets.IntSlider(min=0, max=5, step=1, value=0, description=\"Sample\"),\n",
    "         time_step=widgets.IntSlider(min=0, max=9, step=1, value=0, description=\"Time Step\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZAjfZy_7d5NU",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "\n",
    "> Note: CO₂ and SO₂ are global values with no spatial variation, so we omit them from the plots for clarity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MPzl9SVGZCw9",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 5.2: Spatiotemporal Climate Data Explorer  \n",
    "\n",
    "This widget visualizes the evolution of **CO₂, CH₄, SO₂, and Black Carbon** over **10 years** across a **96×144 global grid**.  \n",
    "\n",
    "- **Use the slider** to navigate through time.  \n",
    "- **Color scale**: **Red = High**, **Blue = Low**.  \n",
    "- **Observe spatial patterns**: Identify emission hotspots, variations, and trends.  \n",
    "\n",
    "This helps understand how climate variables evolve over space and time, crucial for predictive modeling. 🚀  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KzL1vMMnZCw9",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Make sure you execute this cell to enable the widget!<br>\n",
    "# @markdown * Click on the arrow Use the control buttons(`<`,`<<`,`>`,`>>`)  to anable the  animation\n",
    "animate_climate_variables(X_train_torch, sample_idx=0, scale_mode='auto')  #May toggle scale_mode toscale_mode='auto'or scale_mode='log'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G9EDNtCQnzq6",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Interpreting the Animation\n",
    "\n",
    "This animation displays the spatial distribution of four climate variables (CO₂, CH₄, SO₂, Black Carbon) side by side over a 10-year sequence. Each panel corresponds to one variable, and the colors represent its relative concentration across the globe for each year. As the animation progresses, you can observe how the spatial patterns of each variable evolve over time.\n",
    "\n",
    "### How to use the animation\n",
    "Use the control buttons(`<`,`<<`,`>`,`>>`) given to start the animation.\n",
    "\n",
    "Simply watch the animation. It automatically cycles through the 10-year period, updating all four panels simultaneously. Focus on how patterns change across years and compare differences between variables. No manual interaction is required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CnBIyM7JdD6I",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Section_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9k6hl90KZCw9",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 6: Defining the CNN-LSTM Model Architecture\n",
    "\n",
    "🚀 **Next Step:** Moving to Defining the CNN-LSTM Model Architecture!\n",
    "\n",
    "The Challenge: Modeling Spatiotemporal Climate Data\n",
    "Climate data exhibits both **spatial patterns** (e.g., temperature distributions) and **temporal dependencies** (e.g., seasonal trends). Standard deep learning models struggle to effectively capture both aspects.\n",
    "\n",
    "To overcome this, we use a **hybrid CNN-LSTM architecture**, which is **specially designed** to process spatiotemporal data.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nnTh9BxDVBKa",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "## Section 6.1 Understanding the CNN-LSTM Hybrid Model\n",
    "\n",
    "The CNN-LSTM model integrates two powerful deep learning components:\n",
    "\n",
    "1. Convolutional Neural Networks (CNNs): Extracting Spatial Features\n",
    "- CNNs are highly effective at recognizing spatial structures in images.\n",
    "- Here, CNNs are applied **independently at each time step** to extract spatial patterns from climate data.\n",
    "- This ensures that each time step has a structured representation before moving forward in the model.\n",
    "\n",
    "2. Long Short-Term Memory Networks (LSTMs): Capturing Temporal Dependencies\n",
    "- LSTMs are specialized for handling **sequential data**, such as time series.\n",
    "- After spatial features are extracted by CNNs, the LSTM learns **how these features evolve over time**.\n",
    "- This enables the model to understand long-term climate trends.\n",
    "\n",
    "---\n",
    "\n",
    "**Model Workflow: Step-by-Step**  \n",
    "1️⃣ **Input Data:** Climate data is structured as a sequence of frames, where each frame represents a climate variable snapshot.  \n",
    "2️⃣ **CNN Feature Extraction:** A CNN is applied to **each time step** to extract spatial features.  \n",
    "3️⃣ **Time-Distributed Processing:** The CNN operates in a **Time Distributed** manner, ensuring spatial features remain distinct at each time step.  \n",
    "4️⃣ **LSTM Sequence Learning:** The extracted spatial features are passed into an LSTM, which captures **temporal dependencies**.  \n",
    "5️⃣ **Prediction Output:** The model predicts climate variables for the next time step, enabling forecasting.  \n",
    "\n",
    "---\n",
    "\n",
    "###  **Why Use Time-Distributed Layers?**\n",
    "🔹 Without a **Time-Distributed Layer**, CNNs would treat the entire sequence as a single image, losing temporal information.  \n",
    "🔹 With **Time-Distributed CNN**, spatial features are extracted **independently at each time step** before entering the LSTM.  \n",
    "🔹 This preserves both spatial and temporal relationships, leading to **better climate forecasting accuracy**.\n",
    "\n",
    "📖 **Further Reading:**  \n",
    "👉 [Understanding Time-Distributed Layers in Deep Learning](https://medium.com/smileinnovation/how-to-work-with-time-distributed-data-in-a-neural-network-b8b39aa4ce00)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7iWzhwItVFZh",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "## Section 6.2 Implementing the CNN-LSTM Model in PyTorch\n",
    "\n",
    "\n",
    "Before diving into the code, let’s understand its structure and the tools we will use.\n",
    "\n",
    "**What Are We Doing?**\n",
    "\n",
    "We are implementing the CNN-LSTM architecture using PyTorch, which will:\n",
    "- **Extract spatial features** from climate data using CNN layers.\n",
    "- **Preserve time-step independence** with Time-Distributed layers.\n",
    "- **Model sequential dependencies** using an LSTM.\n",
    "- **Output a structured climate forecast**.\n",
    "\n",
    "**Why Use PyTorch?**\n",
    "\n",
    "PyTorch provides:\n",
    "- **Flexibility**: Easy debugging and dynamic computation graphs.\n",
    "- **Modular Design**: Enables clean implementation of CNN and LSTM layers.\n",
    "- **Efficient Computation**: Optimized tensor operations for large climate datasets.\n",
    "\n",
    "**How Will We Implement This in Code ?**\n",
    "\n",
    "We will define a PyTorch model that:\n",
    "- Uses `nn.Conv2d` for spatial feature extraction.\n",
    "- Applies **Time-Distributed processing** using reshaping techniques.\n",
    "- Employs `nn.LSTM` for capturing temporal dependencies.\n",
    "- Outputs climate forecasts using `nn.Linear` and `nn.Reshape`.\n",
    "\n",
    "<details>\n",
    "<summary> <font color='lightGreen'>Recap Key PyTorch Functions and Methods to be Used in the Next code cell </font></summary>\n",
    "\n",
    " **Recap Key PyTorch Functions and Methods to be Used in the Next code cell**\n",
    "\n",
    "| **Component**         | **PyTorch Function**              | **Purpose**                                   |\n",
    "|----------------------|--------------------------------|---------------------------------------------|\n",
    "| **CNN Layer**       | `nn.Conv2d()`                   | Extracts spatial features                   |\n",
    "| **Pooling Layer**   | `nn.AvgPool2d()`                | Reduces spatial dimensions                  |\n",
    "| **Flattening**      | `torch.flatten()`               | Prepares data for LSTM input                |\n",
    "| **LSTM Layer**      | `nn.LSTM()`                     | Captures temporal dependencies              |\n",
    "| **Fully Connected** | `nn.Linear()`                   | Outputs predictions                         |\n",
    "| **Activation**      | `nn.ReLU()`, `nn.Linear()`      | Introduces non-linearity, maps predictions  |\n",
    "</details>\n",
    "\n",
    "With this understanding, let’s now define the model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7zjZmmlYZCw-",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class TimeDistributed(nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper to apply a given module independently to each time step.\n",
    "    This is useful for processing sequential climate data with CNN layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, module):\n",
    "        \"\"\"\n",
    "        Initializes the TimeDistributed wrapper.\n",
    "\n",
    "        Args:\n",
    "        module (nn.Module): The neural network layer (e.g., Conv2D, Pooling)\n",
    "                            to be applied across time steps.\n",
    "        \"\"\"\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module  # Store the provided module\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass that applies the module independently across time steps.\n",
    "\n",
    "        Args:\n",
    "        x (tensor): Input of shape (batch_size, time_steps, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "        tensor: Output tensor with the same time dimension preserved.\n",
    "        \"\"\"\n",
    "        batch_size, time_steps = x.size(0), x.size(1)  # Extract batch and time steps\n",
    "        input_size = tuple(x.size()[2:])  # Get spatial dimensions (C, H, W)\n",
    "\n",
    "        # Flatten time steps into the batch dimension for independent processing\n",
    "        x = x.contiguous().view(batch_size * time_steps, *input_size)\n",
    "\n",
    "        # Apply the module (e.g., CNN) separately to each time step\n",
    "        x = self.module(x)\n",
    "\n",
    "        # Restore original (batch, time, ...) structure after processing\n",
    "        output_size = tuple(x.size()[1:])  # Extract new feature dimensions\n",
    "        return x.view(batch_size, time_steps, *output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zVlrg4TQZCw-",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<details>\n",
    "<summary> <font color='lightGreen'> Breaking Down the Code </font></summary>\n",
    "\n",
    " 🔍 **Understanding the `TimeDistributed` Wrapper**  \n",
    "\n",
    "Neural networks process data in a structured way, and PyTorch provides tools to make this easier. In sequence-based tasks, like climate forecasting, we often need to **apply the same neural network layer to each time step separately**. This is where the **`TimeDistributed` wrapper** comes in.  \n",
    "\n",
    "Normally, CNNs process entire images at once, but in time-series tasks, we need CNNs to **extract features from each timestep independently** before passing them to an LSTM. The `TimeDistributed` class helps achieve this by **automatically applying CNNs or other layers to each time step separately** while keeping the time structure intact.  \n",
    "\n",
    "**Step-by-Step Breakdown**  \n",
    "\n",
    "🔹 **Defining the Class (`__init__()`)**  \n",
    "- In Python, a **class** is like a **blueprint** for creating reusable components.  \n",
    "- This class **inherits from `nn.Module`**, which is a standard PyTorch way to define neural network layers.  \n",
    "- The `TimeDistributed` wrapper takes a **PyTorch layer (like `Conv2D`)** as input and ensures that it is applied **individually to each time step** in a sequence.  \n",
    "\n",
    "🔹 **How Data Moves Through `forward()`**  \n",
    "1️⃣ The **batch size and number of time steps** are extracted from the input tensor.  \n",
    "2️⃣ The input is **reshaped** so that all timesteps are treated as separate images.  \n",
    "3️⃣ The selected PyTorch **module (e.g., CNN)** is applied to the reshaped data.  \n",
    "4️⃣ The output is then **reshaped back** into its original format, keeping time steps intact.  \n",
    "\n",
    "✅ **Key Benefit**: This method allows CNNs to extract spatial features **without losing the sequence structure**, making it ready for LSTM processing.  \n",
    "\n",
    "By combining **CNNs (for spatial feature extraction) and LSTMs (for temporal learning)**, we ensure that the model learns both **where** and **how** climate patterns change over time.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EhZNTrFzZCw-",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Section 6.2.1. Building the CNN-LSTM Model  \n",
    "Now, we define the full **CNN-LSTM model**, integrating:  \n",
    "- **`TimeDistributed CNN`** to extract spatial features per timestep.  \n",
    "- **`LSTM`** to learn temporal patterns.  \n",
    "- **`Fully Connected Layer`** to predict climate variables.  \n",
    "\n",
    "**Key Components in The following  Code cell:**  \n",
    "- **`TimeDistributed`**: Applies CNN layers independently to each timestep.  \n",
    "- **`nn.LSTM`**: Captures long-term dependencies across timesteps.  \n",
    "- **`nn.Linear`**: Maps LSTM outputs to climate predictions.  \n",
    "- **Reshaping techniques**: Ensures correct data format before feeding into layers.  \n",
    "\n",
    "### Section 6.2.2 Implementing TimeDistributed Wrapper\n",
    "   \n",
    "Before integrating CNN into our model, we define a **TimeDistributed wrapper** to ensure CNN processes each timestep separately.\n",
    "\n",
    "<details>\n",
    "<summary> <font color='lightGreen'>Understanding the `TimeDistributed` Wrapper</font></summary>\n",
    "** Understanding the `TimeDistributed` Wrapper**\n",
    "\n",
    "When working with **climate data**, each time step contains **spatial information** that must be processed separately before considering temporal dependencies. A CNN alone does not recognize time steps—it treats inputs as independent images. We need a way to **apply CNN layers individually at each time step** before passing the output to an LSTM.\n",
    "\n",
    "---\n",
    "\n",
    " 🏗 ***Why Do We Need `TimeDistributed`?****\n",
    "❌ **Without `TimeDistributed`**:  \n",
    "- A CNN would **see all time steps as one large image**, ignoring the temporal structure.\n",
    "\n",
    "✅ **With `TimeDistributed`**:  \n",
    "- The CNN applies its filters **individually** to each time step.\n",
    "- This ensures the **spatial features are extracted separately** before temporal modeling.\n",
    "\n",
    "---\n",
    "</details>\n",
    "\n",
    "Let's now define the model! 🚀  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WqCjGjE7ZCw-",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# --- Define helper modules ---\n",
    "class TimeDistributed(nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper to apply a given module independently to each time step.\n",
    "    This is useful for processing sequential climate data with CNN layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, module):\n",
    "        \"\"\"\n",
    "        Initializes the TimeDistributed wrapper.\n",
    "\n",
    "        Args:\n",
    "        module (nn.Module): The neural network layer (e.g., Conv2D, Pooling)\n",
    "                            to be applied across time steps.\n",
    "        \"\"\"\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module  # Store the provided module\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass that applies the module independently across time steps.\n",
    "\n",
    "        Args:\n",
    "        x (tensor): Input of shape (batch_size, time_steps, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "        tensor: Output tensor with the same time dimension preserved.\n",
    "        \"\"\"\n",
    "        batch_size, time_steps = x.size(0), x.size(1)  # Extract batch and time steps\n",
    "        input_size = tuple(x.size()[2:])  # Get spatial dimensions (C, H, W)\n",
    "\n",
    "        # Flatten time steps into the batch dimension for independent processing\n",
    "        x = x.contiguous().view(batch_size * time_steps, *input_size)\n",
    "\n",
    "        # Apply the module (e.g., CNN) separately to each time step\n",
    "        x = self.module(x)\n",
    "\n",
    "        # Restore original (batch, time, ...) structure after processing\n",
    "        output_size = tuple(x.size()[1:])  # Extract new feature dimensions\n",
    "        return x.view(batch_size, time_steps, *output_size)\n",
    "\n",
    "\n",
    "\n",
    "class ReshapeLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom PyTorch layer to reshape a tensor into a structured spatial format.\n",
    "    Ensures the model outputs a climate prediction grid of shape (batch, 1, height, width).\n",
    "    \"\"\"\n",
    "    def __init__(self, shape):\n",
    "        \"\"\"\n",
    "        Initializes the reshape layer.\n",
    "\n",
    "        Args:\n",
    "        shape (tuple): The target shape (excluding batch size), e.g., (1, 96, 144).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.shape = shape  # Store the target shape for reshaping\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass for reshaping.\n",
    "\n",
    "        Args:\n",
    "        x (tensor): The input tensor of shape (batch, features).\n",
    "\n",
    "        Returns:\n",
    "        tensor: Reshaped tensor with dimensions (batch, 1, height, width).\n",
    "        \"\"\"\n",
    "        return x.view(-1, *self.shape)  # Reshape while preserving batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dd5b1d-7495-43a5-997b-d5c1e687715f",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 6.2.2: Implementing the forward pass of CNN_LSTM model, which is designed for spatiotemporal climate forecasting.\n",
    "\n",
    "\n",
    "In this exercise, you'll implement the complete `forward` method of the `CNN_LSTM` model for spatiotemporal climate forecasting.\n",
    "\n",
    "This model:\n",
    "- Applies CNN layers independently to each timestep using `TimeDistributed` wrappers.\n",
    "- Uses an LSTM to capture temporal dependencies from the CNN-extracted spatial features.\n",
    "- Applies a fully connected (linear) layer to project the LSTM output to a spatial climate grid.\n",
    "- Reshapes the output to a 2D spatial prediction map of shape `(1, 96, 144)` per batch.\n",
    "\n",
    "You will use the following layers in the same order:\n",
    "1. `self.time_distributed_conv` - for convolution over time-distributed spatial input.\n",
    "2. `self.time_distributed_pool` - for reducing spatial resolution.\n",
    "3. `self.time_distributed_global_pool` - for compressing feature maps to size (1,1).\n",
    "4. `squeeze` - to remove redundant spatial dimensions after pooling.\n",
    "5. `self.lstm` - to process the sequence of CNN features.\n",
    "6. `self.fc` - to map to the flattened spatial map (96x144).\n",
    "7. `self.reshape` - to reshape the output to (batch, 1, 96, 144).\n",
    "\n",
    "📌 **Input shape:** `(batch, timesteps, channels, height, width)`  \n",
    "📌 **Expected output shape:** `(batch, 1, 96, 144)`\n",
    "🟨 2. Exercise Code C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0703bd6c-2644-4c86-9118-f8c0e49a1c93",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# --- Define the CNN-LSTM Model ---\n",
    "class CNN_LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN-LSTM Model for spatiotemporal climate forecasting.\n",
    "\n",
    "    - CNN (TimeDistributed) extracts spatial features per timestep.\n",
    "    - LSTM captures temporal dependencies from CNN-extracted features.\n",
    "    - Fully Connected Layer maps LSTM output to a climate prediction grid.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize CNN-LSTM model layers.\"\"\"\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "\n",
    "        # CNN feature extraction applied to each timestep independently\n",
    "        self.time_distributed_conv = TimeDistributed(nn.Conv2d(in_channels=4, out_channels=20, kernel_size=3, padding=1))\n",
    "        self.time_distributed_pool = TimeDistributed(nn.AvgPool2d(2))  # Reduces spatial size\n",
    "        self.time_distributed_global_pool = TimeDistributed(nn.AdaptiveAvgPool2d((1, 1)))  # Compresses feature maps\n",
    "\n",
    "        # LSTM processes extracted spatial features across time\n",
    "        self.lstm = nn.LSTM(input_size=20, hidden_size=25, batch_first=True)\n",
    "\n",
    "        # Fully connected layer maps LSTM outputs to flattened spatial grid (96x144)\n",
    "        self.fc = nn.Linear(25, 96 * 144)\n",
    "\n",
    "        # Reshape layer formats output to match spatial map dimensions\n",
    "        self.reshape = ReshapeLayer((1, 96, 144))\n",
    "\n",
    "     # Specify the computations performed on the data\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines forward pass for data flow through the CNN-LSTM model.\n",
    "\n",
    "        Args:\n",
    "          x (tensor): Input tensor of shape (batch, timesteps, channels, height, width)\n",
    "\n",
    "        Returns:\n",
    "          tensor: Output prediction map of shape (batch, 1, 96, 144)\n",
    "        \"\"\"\n",
    "        batch_size, timesteps, C, H, W = x.size()\n",
    "\n",
    "        #################################################\n",
    "        ## TODO for students: complete the forward pass ##\n",
    "        # 1. Apply TimeDistributed Conv2D layer to extract features from each timestep.\n",
    "        # 2. Reduce the spatial dimensions using average pooling.\n",
    "        # 3. Apply global average pooling to compress each feature map to size (1,1).\n",
    "        # 4. Squeeze out the singleton spatial dimensions.\n",
    "        # 5. Pass the resulting sequence of features to the LSTM.\n",
    "        # 6. Use the last output of the LSTM.\n",
    "        # 7. Project to flattened grid using the fully connected layer.\n",
    "        # 8. Reshape the output to (batch, 1, 96, 144) spatial format.\n",
    "        raise NotImplementedError(\"Student exercise: complete the CNN-LSTM forward pass\")\n",
    "        #################################################\n",
    "\n",
    "        # Apply convolution over each timestep\n",
    "        x = self.time_distributed_conv(...)  # <-- Replace ... with input tensor\n",
    "\n",
    "        # Reduce spatial resolution\n",
    "        x = self.time_distributed_pool(...)  # <-- Pass the result of previous layer\n",
    "\n",
    "        # Compress feature maps to (1,1) spatial dimensions\n",
    "        x = self.time_distributed_global_pool(...)  # <-- Pass the pooled output\n",
    "\n",
    "        # Remove (1,1) spatial dimensions\n",
    "        x = x.squeeze(-1).squeeze(-1)  # <-- Output now (batch, timesteps, features)\n",
    "\n",
    "        # Process the sequence with LSTM\n",
    "        r_out, (h_n, c_n) = self.lstm(...)  # <-- Feed squeezed output to LSTM\n",
    "\n",
    "        # Use the final LSTM time step\n",
    "        r_out = r_out[:, -1, :]  # <-- Take last timestep’s output\n",
    "\n",
    "        # Map to spatial output\n",
    "        x = self.fc(...)  # <-- Apply FC to final LSTM output\n",
    "\n",
    "        # Reshape to final map\n",
    "        x = self.reshape(...)  # <-- Reshape to (batch, 1, 96, 144)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139cd239-a9a3-488e-aeb5-ac4235d4b2c7",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/ClimateMatchAcademy/course-content/tree/main/tutorials/W2D4_AIandClimateChange/solutions/W2D4_Tutorial7_Solution_4739ec91.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fZbuAligWbz",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# test your solution\n",
    "\n",
    "# Uncomment to test the output shape\n",
    "#model = CNN_LSTM()  # Instantiate Model\n",
    "# Generate random test input of shape (batch=2, timesteps=5, channels=4, height=32, width=48)\n",
    "#x_test = torch.randn(2, 5, 4, 32, 48)\n",
    "#output = model(x_test) # Run forward pass\n",
    "# Check output shape\n",
    "#print(\"Output shape:\", output.shape)  # Should print: torch.Size([2, 1, 96, 144])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3tEz106tZCw_",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<details>\n",
    "<summary> <font color='lightGreen'> Breaking Down the Code </font></summary>\n",
    "\n",
    " 🔍 **Understanding the `CNN_LSTM` Model**  \n",
    "\n",
    "Deep learning models in PyTorch are built using **classes that inherit from `nn.Module`**, which acts as a blueprint for defining and organizing model components. PyTorch provides a special method called `forward()`, which specifies how input data flows through the network. Instead of manually applying layers step by step, `forward()` ensures that the right computations happen in the correct order automatically.  \n",
    "\n",
    " **Step-by-Step Explanation of the Model**  \n",
    "\n",
    "🔹 **Defining the Model (`__init__()`)**  \n",
    "- We use **`TimeDistributed` CNN layers** to process each timestep independently. CNNs help extract spatial patterns, such as temperature or pressure variations in climate data.  \n",
    "- **Pooling layers** (`AvgPool2d`, `AdaptiveAvgPool2d`) reduce the size of feature maps while keeping essential information, making the model efficient.  \n",
    "- The **LSTM (`nn.LSTM`)** takes these features over time and learns how climate patterns evolve.  \n",
    "- Finally, a **fully connected layer (`Linear`)** converts the LSTM outputs into a **climate forecast grid** for prediction.  \n",
    "\n",
    "🔹 **How Data Flows Through the Model (`forward()`)**  \n",
    "1️⃣ The input first moves through the **CNN layers** inside `TimeDistributed`, ensuring that each timestep is processed separately.  \n",
    "2️⃣ The CNN output is **flattened**, meaning unnecessary spatial dimensions are removed, keeping only the most important information.  \n",
    "3️⃣ This processed data is then passed to the **LSTM**, which learns how the extracted climate features change over time.  \n",
    "4️⃣ The **last LSTM output** is selected because it contains the most useful information for prediction.  \n",
    "5️⃣ Finally, the **fully connected layer** generates a numerical output, which is reshaped into a **spatial climate map**—a structured prediction of climate variables.  \n",
    "\n",
    "By combining **CNNs (for spatial feature extraction)** and **LSTMs (for tracking changes over time)**, this model effectively learns both **where** and **how** climate patterns evolve.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WSZ6uuPyZCw_",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Section 6.2.4 Final Step: Instantiating and Checking Model\n",
    "Before training, let’s instantiate our model and check its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1YnZq0MLZCw_",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Instantiate Model\n",
    "model = CNN_LSTM()\n",
    "\n",
    "# Print Model Summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cJsyfmmWZCw_",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "🔎 What do you observe in the architecture?\n",
    "\n",
    "The model follows a structured approach where CNN layers process spatial features independently for each time step before LSTMs capture temporal relationships. Notably, the use of `TimeDistributed` ensures that convolutional layers operate consistently across time, maintaining spatial integrity. The final fully connected layer acts as a bridge between the sequential features learned by the LSTM and the structured climate prediction output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P7xnz7NodH-5",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Section_6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1jbub84UZCxA",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 7: Defining the Model, Loss Function, and Optimizer\n",
    "\n",
    "With the training data ready, we now define the core components needed for model training:\n",
    "\n",
    "- **The Model:** A CNN-LSTM hybrid to learn spatiotemporal climate patterns.\n",
    "- **The Loss Function:** Measures how well the model predicts climate variables.\n",
    "- **The Optimizer:** Updates model parameters to minimize error.\n",
    "\n",
    "---\n",
    "<details>\n",
    "<summary> <font color='lightGreen'>Section 7.1 Defining the Model</font></summary>\n",
    "\n",
    "## Section 7.1 Defining the Model\n",
    "\n",
    "We initialize an instance of `CNN_LSTM`, which consists of:\n",
    "\n",
    "1. **CNN Layers (TimeDistributed Conv2D):** Extracts spatial features from climate data at each time step.\n",
    "2. **LSTM:** Captures long-term dependencies across time steps.\n",
    "3. **Fully Connected Layer:** Maps LSTM outputs to the climate variable predictions.\n",
    "\n",
    "---\n",
    "</details>\n",
    "<details>\n",
    "<summary> <font color='lightGreen'>Section 7.2 Defining the Loss Function (Mean Squared Error)</font></summary>\n",
    "\n",
    "## Section 7.2 Defining the Loss Function (Mean Squared Error)\n",
    "\n",
    "We use **Mean Squared Error (MSE)** as our loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_{\\text{pred}, i} - y_{\\text{true}, i})^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- \\( N \\) is the number of grid points.\n",
    "- \\( y_{\\text{pred}} \\) is the model's predicted climate variable.\n",
    "- \\( y_{\\text{true}} \\) is the ground truth.\n",
    "- The function **penalizes larger errors more strongly**, stabilizing model learning.\n",
    "\n",
    "---\n",
    "</details>\n",
    "<details>\n",
    "<summary> <font color='lightGreen'>Section 7.3 Defining the Optimizer (Adam Algorithm)</font></summary>\n",
    "\n",
    "## Section 7.3 Defining the Optimizer (Adam Algorithm)\n",
    "\n",
    "The **Adam optimizer** updates model parameters by computing adaptive learning rates for each weight:\n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} - \\frac{\\eta}{\\sqrt{v_t} + \\epsilon} m_t\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- \\( g_t \\) is the gradient at time \\( t \\).\n",
    "- \\( m_t \\) and \\( v_t \\) are moving averages of gradients and squared gradients.\n",
    "- \\( \\beta_1 = 0.9 \\) and \\( \\beta_2 = 0.999 \\) control momentum.\n",
    "- \\( \\eta \\) is the learning rate (set to **0.001**).\n",
    "- **Advantages:** Adam adapts learning rates dynamically, making it robust for deep learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qfJs9xiFZCxA",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# 7. Define Model, Loss Function, and Optimizer\n",
    "\n",
    "# Initialize the CNN-LSTM model\n",
    "cnn_model = CNN_LSTM()\n",
    "print(cnn_model)  # Display model architecture\n",
    "\n",
    "# Define Mean Squared Error (MSE) Loss for regression-based climate prediction\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Adam optimizer with an adaptive learning rate (default β₁=0.9, β₂=0.999)\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Print key components\n",
    "print(f\"Loss Function: {criterion}\")\n",
    "print(f\"Optimizer: {optimizer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZZdvCD7jZCxA",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We saw the **CNN-LSTM model** architecture, where a **TimeDistributed CNN** extracts spatial patterns, an **LSTM** captures temporal dependencies, and a **fully connected layer** reconstructs spatial grids.  \n",
    "\n",
    "✔ **Loss Function:** MSE minimizes prediction errors by penalizing squared differences.  \n",
    "✔ **Optimizer:** Adam ensures efficient training with adaptive learning rates:  \n",
    "\n",
    "With these components defined, we can now train the model and evaluate its performance on climate data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0KAYgNKdMNG",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Section_7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JkC0W7jSZCxA",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 8: Training Loop with Validation and Early Stopping\n",
    "\n",
    "Now that we have defined our model, optimizer, and loss function, we are ready to train the network.\n",
    "Training a deep learning model involves iterating over the dataset multiple times, adjusting the model's parameters to minimize error.\n",
    "To ensure the model generalizes well to unseen data, we implement **validation** and **early stopping** during training.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zMqK6kedW0jZ",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 8.1 Understanding the Training Process  \n",
    "\n",
    "Each deep learning model is trained using a loop that performs the following steps:  \n",
    "\n",
    "1. **Forward Pass** – The model processes the input and makes predictions.  \n",
    "2. **Loss Computation** – The difference between predictions and actual values is calculated using a loss function.  \n",
    "3. **Backward Pass (Backpropagation)** – The gradients of the loss function with respect to the model's parameters are computed.  \n",
    "4. **Parameter Update** – The optimizer updates the model parameters using these gradients.  \n",
    "5. **Validation** – After each training epoch, the model is evaluated on a separate validation set to check its generalization ability.  \n",
    "6. **Early Stopping** – Training is stopped when the validation loss stops improving to prevent overfitting.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Fu5TRT3qW2h9",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "## Section 8.2 Key Hyperparameters in Training  \n",
    "\n",
    "- **Epochs**: The number of times the model goes through the entire training dataset.  \n",
    "- **Batch Size**: Number of samples processed before the model's weights are updated.  \n",
    "- **Learning Rate**: Determines how much the model’s weights are adjusted during each optimization step.  \n",
    "- **Patience**: The number of epochs to wait before stopping if validation loss doesn't improve.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "akdkIMv4W36b",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "## Section 8.3 Preparing for Training  \n",
    "\n",
    "We first split the dataset into **training** and **validation** sets. This allows us to monitor the model’s performance on unseen data during training.\n",
    "We also set up the optimizer and early stopping parameters.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MlG17kw-ZCxB",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Early stopping parameters\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Split into training and validation sets\n",
    "batch_size = 16\n",
    "train_size = int(0.8 * len(X_train_torch))\n",
    "val_size = len(X_train_torch) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    torch.utils.data.TensorDataset(X_train_torch, Y_train_torch),\n",
    "    [train_size, val_size]\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eiPm1YWZCxB",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 8.4 Training Loop with Early Stopping  \n",
    "\n",
    "The training loop processes the dataset in mini-batches. The model learns by computing gradients and updating parameters.\n",
    "After each epoch, we evaluate the model on the validation set and apply early stopping.  \n",
    "\n",
    " Section : The Training Loop\n",
    "\n",
    "Now, let's implement the core training loop. The loop performs the following steps:\n",
    "\n",
    "1. **Set the model to training mode** – This ensures that certain layers (like dropout) behave correctly.\n",
    "2. **Iterate through the training batches** – The model processes small subsets of data at a time.\n",
    "3. **Compute predictions** – The model generates output for each input batch.\n",
    "4. **Calculate the loss** – The difference between predicted and actual values is computed.\n",
    "5. **Perform backpropagation** – Gradients are computed and used to update the model’s weights.\n",
    "6. **Track validation performance** – After each epoch, the model is evaluated on the validation set.\n",
    "7. **Apply early stopping** – Training stops if validation loss does not improve for a predefined number of epochs.\n",
    "\n",
    "Let's implement this step by step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333c3f99-0ade-4ac0-838c-4fc9e7dfa0a5",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 8.4: Training a CNN with weight and gradient tracking\n",
    "\n",
    "Implement a PyTorch training loop for a CNN. Track the weights and gradients after every batch for analysis.\n",
    "\n",
    "You are given: a model (`cnn_model`), loss function (`criterion`), optimizer, and data loaders. Your task:\n",
    "\n",
    "- Run forward pass on batches\n",
    "- Compute loss and backpropagate\n",
    "- Update model weights\n",
    "- Record weights and gradients each batch\n",
    "- Compute training and validation losses per epoch\n",
    "- Apply early stopping after `patience` epochs with no val improvement\n",
    "\n",
    "**Hint**: Use `model.train()`, `optimizer.zero_grad()`, `loss.backward()`, and `optimizer.step()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc55979-3119-4b8f-b863-967b936ef8ea",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def train_model(cnn_model, train_loader, val_loader, optimizer, criterion, num_epochs, patience):\n",
    "    \"\"\"Train a CNN model and store weights/gradients per batch\n",
    "\n",
    "    Args:\n",
    "        cnn_model (nn.Module): the model to train\n",
    "        train_loader (DataLoader): dataloader for training data\n",
    "        val_loader (DataLoader): dataloader for validation data\n",
    "        optimizer (torch.optim.Optimizer): optimizer\n",
    "        criterion (nn.Module): loss function\n",
    "        num_epochs (int): total number of training epochs\n",
    "        patience (int): early stopping patience\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_losses, val_losses, weights_history, grads_history)\n",
    "    \"\"\"\n",
    "\n",
    "    #################################################\n",
    "    # TODO:Implement the training loop with weight/gradient tracking\n",
    "    # 1. Forward pass using X_batch\n",
    "    # 2. Compute loss between predictions and targets\n",
    "    # 3. Backpropagate and update weights\n",
    "    # 4. Track weights and gradients for each batch\n",
    "    # Fill remove the following line of code one you have completed the exercise:\n",
    "    raise NotImplementedError(\"Student exercise: implement training loop with weight/grad tracking\")\n",
    "    #################################################\n",
    "\n",
    "    # Initialize trackers for losses and history\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    weights_history = []\n",
    "    grads_history = []\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    print(\"Training Started!\")\n",
    "    for epoch in range(num_epochs):\n",
    "        cnn_model.train()  # Set model to training mode\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for X_batch, Y_batch in train_loader:\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            Y_pred = cnn_model(...)  # Forward pass-X_batch\n",
    "            loss = criterion(...,...)  # Compute loss between Y_pred  Y_batch\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            # Accumulate batch loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Store weights and gradients for each layer\n",
    "            weights_history.append([p.clone().detach().cpu().numpy() for p in cnn_model.parameters()])\n",
    "            grads_history.append([p.grad.clone().detach().cpu().numpy() for p in cnn_model.parameters() if p.grad is not None])\n",
    "\n",
    "        # Compute average training loss\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        cnn_model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_val, Y_val in val_loader:\n",
    "                Y_val_pred = cnn_model(X_val)\n",
    "                val_loss += criterion(Y_val_pred, Y_val).item()\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    print(\"Training Complete!\")\n",
    "    return train_losses, val_losses, weights_history, grads_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ltFvRM1OvsAh",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/ClimateMatchAcademy/course-content/tree/main/tutorials/W2D4_AIandClimateChange/solutions/W2D4_Tutorial7_Solution_ffd6a78b.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xUGAoqpHg2VH",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "## Uncomment below to test your implementation\n",
    "#train_losses, val_losses, weights_history, grads_history = train_model(\n",
    "#     cnn_model, train_loader, val_loader, optimizer, criterion, num_epochs=1, patience=3\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LPPMEg6OsSdx",
   "metadata": {
    "execution": {}
   },
   "source": [
    "> ⚠️ **Note:** Training will take some time depending on the number of epochs.  \n",
    "> 💡 Run the traning and in the meantime, you can explore the remaining sections of the tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e41b465-d058-4154-860a-f4a182c71171",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#call the training function\n",
    "train_losses, val_losses, weights_history, grads_history = train_model(\n",
    "    cnn_model, train_loader, val_loader, optimizer, criterion, num_epochs=1,patience=3\n",
    ") #Please chnage the number of epcoh to atleast 30\n",
    "#Play with more number of epochs/patience, if you want to improve the performance, just that it may take longer to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef6ba8f-f067-4c0c-bfc6-d6bbf3d3d4da",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Loss Trend\n",
    "\n",
    "Does the loss go down as training continues?📉\n",
    "\n",
    "> In the above code cell where `train_model` is called, set `num_epochs = 30`.\n",
    "\n",
    "If loss still does not decrease,one of the  possible reasong could be :\n",
    "- The dataset may be too small\n",
    "\n",
    "Before training, go to **Section 2.1 – Data Retrieval** and from the drop down select:\n",
    "- All Climate Scenarios\n",
    "- All Input Variables\n",
    "\n",
    "Then run the remaing notebook again and watch the loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lH9azKKqZCxC",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<details>\n",
    "<summary> <font color='lightGreen'>Explanation:</font></summary>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "*   **`cnn_model.train()`:** This line sets the model to training mode. This is important because some layers (like dropout) behave differently during training and evaluation.\n",
    "*   **Inner loop:** The inner loop iterates over the training data in batches.\n",
    "*   **`optimizer.zero_grad()`:** This line clears the gradients from the previous iteration. PyTorch accumulates gradients, so we need to clear them before calculating the gradients for the current iteration.\n",
    "*   **`outputs = cnn_model(inputs)`:** This line performs the forward pass, feeding the input data through the model to obtain the predictions.\n",
    "*   **`loss = criterion(outputs, targets)`:** This line calculates the loss between the predictions and the true values.\n",
    "*   **`loss.backward()`:** This line performs the backward pass, calculating the gradients of the loss function with respect to the model's parameters.\n",
    "*   **`optimizer.step()`:** This line updates the model's parameters using the calculated gradients.\n",
    "*   **`cnn_model.eval()`:** This line sets the model to evaluation mode. This is important because some layers (like dropout) behave differently during training and evaluation.\n",
    "*   **`with torch.no_grad():`:** This block disables gradient calculation during validation. We don't need to calculate gradients during validation, so disabling them saves memory and computation time.\n",
    "*   **Early Stopping:** Inside the validation loop, the code checks if the current validation loss is less than the best validation loss seen so far. If it is, the best validation loss is updated, the `epochs_no_improve` counter is reset, and the model's state is saved to disk. If the validation loss doesn't improve for `patience` epochs, the training loop is broken.\n",
    "</details>\n",
    "\n",
    "Now since we trained the model now letr's check the predictions and visualise them to how does the model actually  perform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OwGRT4PbZCxC",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Run this cell to activate Interactive widget {\"run\":\"auto\",\"vertical-output\":true,\"display-mode\":\"form\"}\n",
    "# Interactive Widgets\n",
    "print(\"Use the widgets below to visualize different aspects of training!\")\n",
    "interact(plot_weight_gradients, epoch=IntSlider(min=0, max=len(weights_history)-1, step=1, value=0, description=\"Epoch\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zSY83MJrZCxC",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Run this cell to activate Interactive widget {\"run\":\"auto\",\"display-mode\":\"form\"}\n",
    "# @markdown Sample Predictions Over Time (Slider)\n",
    "interact(plot_predictions1, epoch=IntSlider(min=0, max=9, step=1, value=0, description=\"Sample\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pj_qZ10ZdPgG",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Section_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "__d0OI3eZCxC",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 9: Making Predictions and Visualizing Results\n",
    "\n",
    "Now that we have trained our model, let's use it to make predictions and visualize the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J2-iUaqQXp7r",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 9.1 Making Predictions\n",
    "\n",
    "We can use the trained model to make predictions on new data. For example, we can predict the temperature for a future time period based on the historical data and projected emission scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jzPYW1SqZCxC",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#Example of prediction (using the training data for demonstration)\n",
    "cnn_model.to(device)\n",
    "cnn_model.eval()\n",
    "with torch.no_grad():\n",
    "    X_train_torch = X_train_torch.to(device)  # Move input data to the same device as model\n",
    "    predicted_tas = cnn_model(X_train_torch[:5]).cpu().detach().numpy()  # Move output back to CPU for NumPy\n",
    "print(\"Predictions recorded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xt1C7_zYZCxC",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<details>\n",
    "<summary> <font color='lightGreen'>Explanation: </font></summary>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This code snippet demonstrates how to make predictions using the trained model. It takes the first 5 sequences from the training data (`X_train_torch[:5]`) and feeds them into the model. The `.detach().numpy()` converts the PyTorch tensor to a NumPy array for easier manipulation and visualization.\n",
    "</details>\n",
    "\n",
    "**Important:** In a real-world scenario, you would use *new*, unseen data to evaluate the model's performance and make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dOVCtcAZCxD",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 9.2 Visualizing Results\n",
    "Visualizing the model's predictions is crucial for understanding its performance and identifying potential issues. We can use libraries like `matplotlib` and `cartopy` to plot the predicted temperature and compare it to the actual temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Id1nIGBuZCxD",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Visualization example (plotting the first prediction)\n",
    "plt.imshow(predicted_tas[0].squeeze(), origin='lower', cmap='RdBu_r')\n",
    "plt.colorbar()\n",
    "plt.title('Predicted Temperature Anomaly')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NxWW3WahZCxD",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<details>\n",
    "<summary> <font color='lightGreen'>Explanation </font></summary>\n",
    "**Explanation:**\n",
    "\n",
    "This code uses `matplotlib.pyplot.imshow` to display the first predicted temperature map. `origin='lower'` ensures that the latitude increases from bottom to top in the plot. `cmap='RdBu_r'` sets the color map to a red-blue diverging colormap, which is commonly used for visualizing temperature anomalies. A colorbar is added to the plot to show the range of temperature values.\n",
    "\n",
    "**Further Visualization Ideas:**\n",
    "\n",
    "*   **Plotting actual vs. predicted temperatures:** Create scatter plots or time series plots to compare the model's predictions to the actual temperature values.\n",
    "*   **Mapping the difference (error):** Plot the difference between the predicted and actual temperatures to visualize the model's errors.\n",
    "*   **Using Cartopy for Geographic Projections:** Use `cartopy` to project the data onto a map, providing a more realistic view of the climate patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CBQBPpu-ZCxD",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**0. Using Cartopy for Geographic Projections**:\n",
    "\n",
    "Use cartopy to project the data onto a map, providing a more realistic view of the climate patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bhHcT5QFZCxD",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Run this cell to view plot of `Predicted Temperature Anomaly`\n",
    "\n",
    "# Set up the plot with a PlateCarree projection\n",
    "fig, ax = plt.subplots(subplot_kw={'projection': ccrs.PlateCarree()}, figsize=(8, 5))\n",
    "# Plot the predicted temperature anomaly\n",
    "img = ax.imshow(predicted_tas[0].squeeze(), origin='lower', cmap='RdBu_r', extent=[-180, 180, -90, 90])\n",
    "# Add coastlines for geographic context\n",
    "ax.coastlines()\n",
    "# Add a colorbar\n",
    "plt.colorbar(img, ax=ax, orientation='vertical', label='Temperature Anomaly')\n",
    "# Titles and labels\n",
    "plt.title('Predicted Temperature Anomaly')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6RZGbsP9ZCxD",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**1 Difference (Error) Map**  \n",
    "To see where the model deviates from the actual temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K-3YMDdaZCxD",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Run this cell to visualise the Difference Map\n",
    "\n",
    "\n",
    "actual_tas = Y_train_torch.cpu().numpy()  # Move to CPU and convert to NumPy if it's a tensor\n",
    "# Compute error map between predicted and actual temperature anomalies\n",
    "error_map = (predicted_tas[0, 0] - actual_tas[0]).squeeze()  # Ensure 2D shape\n",
    "\n",
    "# Plot the difference map\n",
    "fig, ax = plt.subplots(subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "img = ax.imshow(error_map, origin='lower', cmap='coolwarm', extent=[-180, 180, -90, 90])\n",
    "\n",
    "# Add coastlines and colorbar\n",
    "ax.coastlines()\n",
    "plt.colorbar(img, ax=ax, orientation='vertical', label=\"Temperature Difference\")\n",
    "plt.title(\"Prediction Error Map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WI7RWj1EZCxD",
   "metadata": {
    "execution": {}
   },
   "source": [
    "🔥 Insight: This highlights over- and under-predictions across the globe.\n",
    "\n",
    "**2. Latitudinal Mean Temperature Profile**  \n",
    "Check how temperature varies across latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Tm2KbF49ZCxE",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Run this cell to visualise Latitudinal Mean Temperature Profile\n",
    "\n",
    "# Compute zonal mean (average over longitudes)\n",
    "predicted_lat_mean = np.mean(predicted_tas[0].squeeze(), axis=1)  # Shape: (96,)\n",
    "actual_lat_mean = np.mean(actual_tas.squeeze(), axis=(0, 2))  # Ensure shape (96,)\n",
    "\n",
    "# Generate latitude array\n",
    "latitudes = np.linspace(-90, 90, predicted_lat_mean.shape[0])  # Shape: (96,)\n",
    "\n",
    "# Ensure both arrays have the same shape\n",
    "print(\"Shapes:\", latitudes.shape, actual_lat_mean.shape, predicted_lat_mean.shape)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(latitudes, actual_lat_mean, label='Actual', linestyle='--', color='blue')\n",
    "plt.plot(latitudes, predicted_lat_mean, label='Predicted', linestyle='-', color='red')\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Temperature Anomaly')\n",
    "plt.title('Latitudinal Mean Temperature Profile')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Pq7x8IEiZCxE",
   "metadata": {
    "execution": {}
   },
   "source": [
    "📈 Insight: Does the model capture temperature variations across latitudes accurately?\n",
    "\n",
    "**3. Histogram of Prediction Errors**  \n",
    "Assess model bias in temperature anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j99B6-iCZCxE",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Run this cell to visualise the Histogram of Prediction Errors\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "# Flatten arrays and compute errors\n",
    "errors = (predicted_tas[0].squeeze() - actual_tas).flatten()\n",
    "\n",
    "plt.hist(errors, bins=30, color='purple', alpha=0.7, edgecolor='black')\n",
    "plt.axvline(x=0, color='black', linestyle='dashed')\n",
    "\n",
    "plt.xlabel('Prediction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Prediction Errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UUKwJO2iZCxE",
   "metadata": {
    "execution": {}
   },
   "source": [
    "📊 Insight: If errors are centered around 0, the model has no systematic bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2z4Z0w3ZCxE",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown 5. Prediction vs. Ground Truth Visualization\n",
    "# Compares model predictions with actual data\n",
    "\n",
    "#  usage (assuming model predictions are available)\n",
    "sample_idx = 0  # Choose an index from your dataset (e.g., the first sample)\n",
    "sample_input = X_train_torch[sample_idx].unsqueeze(0).to(device)  # Assuming X_train_torch holds your input data\n",
    "y_pred_sample = cnn_model(sample_input).detach().cpu().numpy()\n",
    "plot_predictions(Y_train_all[0], y_pred_sample, 'Model Prediction vs. Ground Truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b4caca-3cd8-4b31-b3f9-aa54d220e48d",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Noticed poor or vague predictions above?**\n",
    "Follow these quick steps to improve them:\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 Improve Prediction & Visualization\n",
    "\n",
    "**1. Select all scenarios**\n",
    "In *Section 2.1.1*, choose **all climate scenarios**, then **rerun the notebook**.\n",
    "\n",
    "> More data → better predictions.\n",
    "\n",
    "**2. Set epochs ≥ 30**\n",
    "In the training section, increase epochs to **at least 30**.\n",
    "\n",
    "> Allows the model to learn better.\n",
    "\n",
    "---\n",
    "\n",
    "> Skipping these can lead to weak or misleading results.\n",
    "> Try both steps and see the difference in plots and performance.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OMzUGLSa9vAg",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Questions 9.2: How can we \"trust\" an emulator’s prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nZIzYhjH-BWA",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/ClimateMatchAcademy/course-content/tree/main/tutorials/W2D4_AIandClimateChange/solutions/W2D4_Tutorial7_Solution_e576b924.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trqV-hfUdU9i",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Section_9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P0OmF0cPZCxE",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 10 : Make final prediction on the Test Data from ClimateBench Repository\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8kGBJr9sX5rc",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 10.1 Make prediction for precipitation-related variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gK0FSEDDZCxE",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# Step 1: Load and Preprocess Test Data\n",
    "# ==============================================\n",
    "\n",
    "# Load test data from NetCDF files and compute values\n",
    "download_test_data()\n",
    "\n",
    "#Define the path to the test data\n",
    "test_data_path = \"Data/test/\"\n",
    "\n",
    "X_test = xr.open_mfdataset([data_path + 'inputs_historical.nc',\n",
    "                            test_data_path + 'inputs_ssp245.nc']).compute()\n",
    "\n",
    "# Normalize the selected variables using precomputed mean and standard deviation\n",
    "for var in selected_climate_input_vars:\n",
    "    var_dims = X_test[var].dims  # Get original dimensions of the variable\n",
    "    X_test = X_test.assign({var: (var_dims, normalize(X_test[var].data, var, meanstd_inputs))})  # Apply normalization\n",
    "\n",
    "# Convert data to numpy format suitable for model input\n",
    "X_test_np = input_for_training(X_test, skip_historical=False, len_historical=len_historical)\n",
    "\n",
    "# Convert numpy array to PyTorch tensor and move to the appropriate device (CPU/GPU)\n",
    "X_test_torch = torch.tensor(X_test_np, dtype=torch.float32).to(device)\n",
    "\n",
    "# Fix channel ordering: Move C to the third dimension (B, T, C, H, W)\n",
    "X_test_torch = X_test_torch.permute(0, 1, 4, 2, 3)  # (B, T, C, H, W)\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# Step 2: Make Predictions Using Trained Model (safe version)\n",
    "# ==============================================\n",
    "\n",
    "cnn_model.to(device)\n",
    "cnn_model.eval()\n",
    "\n",
    "# Disable gradient computation\n",
    "predictions = []\n",
    "\n",
    "batch_size = 16  # Small batch size to prevent OOM\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, X_test_torch.shape[0], batch_size):\n",
    "        batch = X_test_torch[i:i+batch_size].to(device)\n",
    "        batch_pred = cnn_model(batch)\n",
    "        predictions.append(batch_pred.cpu())\n",
    "\n",
    "# Concatenate all batches\n",
    "m_pred = torch.cat(predictions).numpy()\n",
    "\n",
    "# ==============================================\n",
    "# Step 3: Reformat Predictions to xarray Format\n",
    "# ==============================================\n",
    "\n",
    "# Reshape predictions to match spatial dimensions (latitude, longitude, time)\n",
    "m_pred = m_pred.reshape(m_pred.shape[0], m_pred.shape[2], m_pred.shape[3])\n",
    "\n",
    "# Convert to xarray DataArray with proper dimensions and coordinates\n",
    "m_pred = xr.DataArray(\n",
    "    m_pred,\n",
    "    dims=['time', 'lat', 'lon'],\n",
    "    coords=[X_test.time.data[slider-1:], X_test.latitude.data, X_test.longitude.data]\n",
    ")\n",
    "\n",
    "# Transpose the data for correct ordering and select relevant time range\n",
    "m_pred = m_pred.transpose('lat', 'lon', 'time').sel(time=slice(2015, 2101)).to_dataset(name=var_to_predict)\n",
    "\n",
    "# Apply unit conversion for precipitation-related variables\n",
    "if var_to_predict in [\"pr90\", \"pr\"]:\n",
    "    m_pred = m_pred.assign({var_to_predict: m_pred[var_to_predict] / 86400})  # Convert from seconds to days\n",
    "\n",
    "flag = 1  # Set to 1 if this cell runs successfully; otherwise, it remains 0. Used in the next cell to check wether to use dummy value or not.\n",
    "\n",
    "print(\"Prediction m_pred is \")\n",
    "m_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1DJkW6UpZCxF",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Run this Cell to See the Plot of the Results {\"run\":\"auto\",\"display-mode\":\"form\"}\n",
    "# Plot results\n",
    "from matplotlib import colors\n",
    "divnorm = colors.TwoSlopeNorm(vmin=-2., vcenter=0., vmax=4.)\n",
    "\n",
    "#the below code sets dummy value to tun th eplots in case the prevous whole cell is commented\n",
    "if flag == 0:\n",
    "    # Dummy prediction: 86 time steps from 2015 to 2100, grid size 96×144\n",
    "\n",
    "    m_pred = xr.DataArray(\n",
    "        data=np.zeros((96, 144, 86)),  # [lat, lon, time]\n",
    "        dims=[\"lat\", \"lon\", \"time\"],\n",
    "        coords={\n",
    "            \"lat\": np.linspace(-90, 90, 96),\n",
    "            \"lon\": np.linspace(0, 360, 144, endpoint=False),\n",
    "            \"time\": np.arange(2015, 2101)\n",
    "        }\n",
    "    ).to_dataset(name=var_to_predict)  # Use your default var name if different\n",
    "    print(\"\\n\\n This is just a dummy plot since the cell above is commented out. To see actual predictions, uncomment the code above and rerun this cell.\\n\\n\\n\")\n",
    "else:\n",
    "    print(\"var_to_predict : \",var_to_predict)\n",
    "\n",
    "\n",
    "\n",
    "for yr in [2020, 2050, 2100]:\n",
    "    f, axes = plt.subplots(1, 1,\n",
    "                           subplot_kw=dict(projection=ccrs.PlateCarree()),\n",
    "                           figsize=(9, 5))\n",
    "\n",
    "    ctr = axes.pcolormesh(m_pred[var_to_predict].sel(time=yr), cmap=\"coolwarm\", norm=divnorm)\n",
    "    plt.colorbar(ctr)\n",
    "    axes.set_title(f\"Estimated {yr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcd0015-484d-43bb-9fa0-3fd0eaf8702f",
   "metadata": {
    "execution": {}
   },
   "source": [
    "> **Note**: For better predictions, select all scenarios in the *Climate Scenarios* dropdown (Section 2.1.1), then rerun the entire notebook.  \n",
    "> More training data generally leads to better accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZamTfMTdZCxF",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 10.2 Run Training and Predictions for Each Target Variable from the `test set` of the Climatebench dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196680a2-5861-43d8-ab95-eac800fb3369",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The section demonstrates how to extend training across multiple target variables (`['tas', 'diurnal_temperature_range', 'pr', 'pr90']`) using the same workflow introduced earlier for single-variable training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BnN97_ZjZCxF",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# #Run Training and Predictions for Each Target Variable from the test set of the Climatebench dataset\n",
    "\n",
    "# #to Load Test Data\n",
    "# download_test_data()\n",
    "\n",
    "# # #choose the target variable  by default it is 'tas'  ; uncomment the commented line below for more target vars\n",
    "# target_vars= ['tas']\n",
    "# #target_vars= ['tas', 'diurnal_temperature_range', 'pr', 'pr90']\n",
    "\n",
    "# # #Define the path to the test data\n",
    "# test_data_path = \"Data/test/\"\n",
    "\n",
    "# # Load and normalize test data\n",
    "# X_test = xr.open_mfdataset([data_path + 'inputs_historical.nc',\n",
    "#                             test_data_path + 'inputs_ssp245.nc']).compute()\n",
    "# for var in selected_climate_input_vars:\n",
    "#     X_test[var].data = normalize(X_test[var].data, var, meanstd_inputs)\n",
    "\n",
    "# X_test_np = input_for_training(X_test, skip_historical=False, len_historical=len_historical)\n",
    "# X_test_tensor = torch.tensor(X_test_np, dtype=torch.float32).to(device)\n",
    "\n",
    "# # Define the model\n",
    "# class CNN_LSTM(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNN_LSTM, self).__init__()\n",
    "#         self.conv = nn.Conv2d(in_channels=4, out_channels=20, kernel_size=3, padding=1)\n",
    "#         self.pool = nn.AvgPool2d(kernel_size=2)\n",
    "#         self.lstm = nn.LSTM(input_size=20 * 48 * 72, hidden_size=25, batch_first=True)\n",
    "#         self.fc = nn.Linear(25, 96 * 144)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         batch_size, seq_len, H, W, C = x.shape  # (batch, time, height, width, channels)\n",
    "\n",
    "#         # Permute to (batch * time, channels, height, width) for Conv2D\n",
    "#         x = x.permute(0, 1, 4, 2, 3).contiguous().view(batch_size * seq_len, C, H, W)\n",
    "\n",
    "#         x = F.relu(self.conv(x))\n",
    "#         x = self.pool(x)\n",
    "\n",
    "#         # Reshape for LSTM: (batch, time, features)\n",
    "#         x = x.view(batch_size, seq_len, -1)\n",
    "\n",
    "#         x, _ = self.lstm(x)  # LSTM expects (batch, seq, features)\n",
    "#         x = self.fc(x[:, -1, :])  # Take last timestep output\n",
    "\n",
    "#         return x.view(-1, 1, 96, 144)  # Reshape to match output shape\n",
    "\n",
    "# # Training loop\n",
    "# for var_to_predict in target_vars:\n",
    "#     print(var_to_predict)\n",
    "\n",
    "#     # Prepare training data\n",
    "#     X_train_all = np.concatenate([input_for_training(X_train_norm[i], skip_historical=skip_flags[i], len_historical=len_historical) for i in range(len(scenario_selected))], axis=0)\n",
    "#     Y_train_all = np.concatenate([output_for_training(Y_train[i], var_to_predict, skip_historical=skip_flags[i], len_historical=len_historical) for i in range(len(scenario_selected))], axis=0)\n",
    "\n",
    "#     # Convert to PyTorch tensors\n",
    "#     X_train_tensor = torch.tensor(X_train_all, dtype=torch.float32).to(device)\n",
    "#     Y_train_tensor = torch.tensor(Y_train_all, dtype=torch.float32).to(device)\n",
    "\n",
    "#     dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "#     dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "#     # Initialize model, loss, and optimizer\n",
    "#     model = CNN_LSTM().to(device)\n",
    "#     optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "#     criterion = nn.MSELoss()\n",
    "#     epochs=1 #change this value to atleast 30\n",
    "\n",
    "#     # Train the model\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         epoch_loss = 0.0\n",
    "#         for X_batch, Y_batch in dataloader:\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(X_batch)\n",
    "#             loss = criterion(outputs, Y_batch.view(-1, 1, 96, 144))\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             epoch_loss += loss.item()\n",
    "#         print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(dataloader):.4f}\")\n",
    "\n",
    "#     # Make predictions\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         m_pred = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "#     # Reshape to xarray\n",
    "#     m_pred = xr.DataArray(m_pred[:, 0, :, :], dims=['time', 'lat', 'lon'],\n",
    "#                           coords=[X_test.time.data[slider-1:], X_test.latitude.data, X_test.longitude.data])\n",
    "#     xr_prediction = m_pred.transpose('lat', 'lon', 'time').sel(time=slice(2015, 2101)).to_dataset(name=var_to_predict)\n",
    "\n",
    "#     if var_to_predict in [\"pr90\", \"pr\"]:\n",
    "#         xr_prediction[var_to_predict] /= 86400\n",
    "\n",
    "#     # Save predictions\n",
    "#     output_path = data_path + f'outputs_ssp245_predict_{var_to_predict}.nc'\n",
    "#     #xr_prediction.to_netcdf(output_path, 'w')\n",
    "#     xr_prediction.close()\n",
    "#     print(f\"Saved {var_to_predict} predictions to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c74158-0162-4cb4-953a-0051b769bc54",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Section_10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7an3PyJzZCxH",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 11: Conclusion\n",
    "\n",
    "Congratulations! You've successfully built and trained a CNN-LSTM model for climate modeling. You've learned how to load and preprocess climate data, define a CNN-LSTM architecture, train the model with validation and early stopping, and visualize the results.\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "*   **Experiment with different architectures:** Try changing the number of layers, the kernel sizes, or the number of LSTM units.\n",
    "*   **Explore different optimizers:** Experiment with different optimization algorithms, such as SGD or RMSProp.\n",
    "*   **Add more features:** Include additional climate variables as input features.\n",
    "*   **Evaluate on a larger dataset:** Train and evaluate the model on a larger and more diverse dataset.\n",
    "*   **Fine-tune hyperparameters:** Use techniques like grid search or random search to find the optimal hyperparameter settings.\n",
    "\n",
    "This notebook provides a solid foundation for exploring the exciting world of deep learning for climate modeling. Keep experimenting, keep learning, and keep building!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cM_lFqHtdeC8",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Section_11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ogNPBoRReXQK",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Summary\n",
    "\n",
    "\n",
    "In this tutorial, we demonstrated how to apply a `CNN-LSTM` deep learning architecture for climate prediction tasks using spatiotemporal climate datasets. Moving beyond traditional machine learning models, we worked directly with raw NetCDF files from the ClimateBench dataset, preserving spatial (`latitude`, `longitude`) and temporal (`year`) structures.\n",
    "\n",
    "We implemented the following core steps:\n",
    "\n",
    "- **Data Preprocessing:** Loaded historical and scenario-based climate variables (`CO₂, CH₄, SO₂, BC`) using `xarray`, applied normalization (`Z-score`), and structured data into overlapping 10-year sliding windows for sequence modeling.\n",
    "\n",
    "- **Model Design:** Built a hybrid `CNN-LSTM` model in PyTorch. Spatial features were extracted independently at each timestep using TimeDistributed `CNNs`, and temporal dependencies were modeled using an `LSTM`. Final outputs were mapped to gridded climate projections.\n",
    "\n",
    "- **Data Preparation for Training:** Converted sequences into PyTorch tensors and aligned dimensions to match model input requirements.\n",
    "\n",
    "This workflow enables learning complex spatiotemporal patterns directly from climate maps, improving long-term climate variable forecasting at the grid level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4D50phKCeQT_",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Resources\n",
    "1. [Neuromatch Deep Learning Course](https://deeplearning.neuromatch.io/tutorials/W1D1_BasicsAndPytorch/student/W1D1_Tutorial1.html)\n",
    "2. [Climatebench baseline model of CNN-LSTM](https://github.com/duncanwp/ClimateBench/blob/main/baseline_models/CNN-LTSM_model.ipynb)\n",
    "3. [how-to-work-with-time-distributed-data-in-a-neural-network]( https://medium.com/smileinnovation/how-to-work-with-time-distributed-data-in-a-neural-network-b8b39aa4ce00)\n",
    "4. [Link to the full `ClimateBench` Dataset](https://zenodo.org/records/7064308)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W2D4_Tutorial7",
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
