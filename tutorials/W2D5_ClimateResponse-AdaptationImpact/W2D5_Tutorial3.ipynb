{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuromatch/climate-course-content/blob/main/tutorials/W2D5_ClimateResponse-AdaptationImpact/W2D5_Tutorial3.ipynb) Â  <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/climate-course-content/main/tutorials/W2D5_ClimateResponse-AdaptationImpact/W2D5_Tutorial3.ipynb\" target=\"_blank\"><img alt=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myt07YFyNgmw"
   },
   "source": [
    "# Tutorial 3:  Testing Model Generalization\n",
    "\n",
    "**Week 2, Day 5, AI and Climate Change**\n",
    "\n",
    "__Content creators:__  Deepak Mewada, Grace Lindsay\n",
    "\n",
    "__Content reviewers:__ Mujeeb Abdulfatai, Nkongho Ayuketang Arreyndip, Jeffrey N. A. Aryee, Paul Heubel, Jenna Pearson, Abel Shibu\n",
    "\n",
    "__Content editors:__ Deepak Mewada, Grace Lindsay\n",
    "\n",
    "__Production editors:__ Konstantine Tsafatinos\n",
    "\n",
    "**Our 2024 Sponsors:** CMIP, NFDI4Earth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDQc1jnoNWcp"
   },
   "source": [
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial:* 25 minutes\n",
    "\n",
    "In this tutorial, you will\n",
    "* Understand the problem of overfitting\n",
    "* Understand generalization\n",
    "* Learn to split data into train and test data\n",
    "* Evaluate trained models on held-out test data\n",
    "* Think about the relationship between model capacity and overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "Vtq0OyoRNPcc"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kwsl6-KNNPcc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# imports:\n",
    "\n",
    "import pandas as pd                                       # For data manipulation\n",
    "from sklearn.model_selection import train_test_split      # For splitting dataset into train and test sets\n",
    "from sklearn.ensemble import RandomForestRegressor        # For Random Forest Regression\n",
    "from sklearn.tree import DecisionTreeRegressor            # For Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Figure Settings\n",
    "import ipywidgets as widgets  # interactive display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\n",
    "    \"https://raw.githubusercontent.com/neuromatch/climate-course-content/main/cma.mplstyle\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gy1MFnohgBkf",
    "outputId": "5b59e43e-acc5-4670-f6d2-12645a585e43"
   },
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed=None):\n",
    "    if seed is None:\n",
    "        seed = np.random.choice(2 ** 32)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    print(f'Random seed {seed} has been set.')\n",
    "\n",
    "# Set a global seed value for reproducibility\n",
    "random_state = 42 # change 42 with any number you like\n",
    "\n",
    "set_seed(seed=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581,
     "referenced_widgets": [
      "82c7600a266d420eb74deb693697c2d5",
      "666b371218354c83b8f01cb2d81a7c00",
      "046054f25dad4c869bd2947642ccd869",
      "16c9961b858a4804b3ad7bdccb57431e",
      "3ac9ce4eed7a4285ae7a367b9923568e",
      "b9b0602a10f9471f8478339ce536ce1b"
     ]
    },
    "id": "ePKqT65QBJSC",
    "outputId": "477d5987-bf40-4a27-9fbc-449e8aec8d9c"
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Testing model generalization\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'gPM64fog-dc'), ('Bilibili', '<video_id_2>'), ('Osf', '<video_id_3>')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wlndBdbV5iJF",
    "outputId": "25084ef3-8417-4a5f-8ebf-0dfec2d90dfc"
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "\n",
    "# @markdown\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import IFrame\n",
    "\n",
    "link_id = \"t48yb\"\n",
    "\n",
    "download_link = f\"https://osf.io/download/{link_id}/\"\n",
    "render_link = f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/{link_id}/?direct%26mode=render%26action=download%26mode=render\"\n",
    "# @markdown\n",
    "out = widgets.Output()\n",
    "with out:\n",
    "    print(f\"If you want to download the slides: {download_link}\")\n",
    "    display(IFrame(src=f\"{render_link}\", width=730, height=410))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ua8XZtQ4_d7b"
   },
   "source": [
    "# Section 1: Model generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siDJE1JScMHU"
   },
   "source": [
    "As discussed in the video, machine learning models can *overfit*. This means they essentially memorize the data points they were trained on. This makes them perform very well on those data points, but when they are presented with data they weren't trained on their predictions are not very good. Therefore, we need to evaluate our models according to how well they perform on data they weren't trained on.\n",
    "\n",
    "To do this, we will split the data into training and testing sets. The training set will be used to train the model, while the testing set will be used to evaluate how well the model performs on unseen data. This helps us ensure that our model can generalize well to new data and avoid overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3l-FpQGjdMvm"
   },
   "source": [
    "## Section 1.1: Load and Prepare the Data\n",
    "\n",
    "As we've learned in the previous tutorial, here we load our dataset and prepare it by removing unnecessary columns and extracting the target variable `tas_FINAL`, representing temperature anomalies in 2050. The anomalies in every case are calculated by subtracting the annual means of the pre-industrial scenario from the annual means of the respective scenario of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "go_ea6wUdysz"
   },
   "outputs": [],
   "source": [
    "# Load and Prepare the Data\n",
    "url_Climatebench_train_val = \"https://osf.io/y2pq7/download\" # Dataset URL\n",
    "training_data = pd.read_csv(url_Climatebench_train_val)  # Load the training data from the provided URL\n",
    "training_data.pop('scenario')  # drop the `scenario` column from the data as it is just a label, but will not be passed into the model.\n",
    "target = training_data.pop('tas_FINAL')  # Extract the target variable 'tas_FINAL' which we aim to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLdPvfbwA4jn"
   },
   "source": [
    "## Section 1.2: Data Splitting for Training and Testing\n",
    "\n",
    "Now, our primary objective is to prepare our dataset for model training and evaluation. To achieve this, we'll utilize the `train_test_split` function from Scikit-learn, which conveniently splits our dataset into training and testing subsets.\n",
    "\n",
    "To facilitate this process, we've imported the essential `train_test_split` function from Scikit-learn earlier in the code:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split      \n",
    "```\n",
    "\n",
    "Our strategy involves randomly allocating 20% of the data for testing purposes, while reserving the remaining 80% for model training. This ensures that our model is evaluated on unseen data, which is crucial for assessing its real-world performance.\n",
    "\n",
    "With this function ready to use, let's seamlessly proceed to split our dataset and go ahead on the journey of model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNuku87mOp5N"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    training_data, target, test_size=0.2, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_RqSUBkuNev"
   },
   "source": [
    "We now have separated the input features (now called `X`) and the target variable (now called `y`) into a training set (`X_train`, `y_train`) and a test set (`X_test`, `y_test`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYgCCj6gBJhO"
   },
   "source": [
    "## Section 1.3: Train a decision tree model on the training data and evaluate it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "sWroxkvmCRcR",
    "outputId": "bc1263db-eb4c-438e-9129-d480edadf75c"
   },
   "outputs": [],
   "source": [
    "# Training the model on the training data\n",
    "dt_regressor = DecisionTreeRegressor(random_state=random_state,max_depth=20)\n",
    "dt_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGztBoOXCesU"
   },
   "source": [
    "Now we will evaluate the model on both the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6cmPJpgSClvz",
    "outputId": "7fb882ba-f5c8-40f8-c9ac-13cdd3003aca"
   },
   "outputs": [],
   "source": [
    "print('Performance on training data:', dt_regressor.score(X_train, y_train))\n",
    "print('Performance on test data    :', dt_regressor.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPbU2D25FX30"
   },
   "source": [
    "We can see here that our model is overfitting: it is performing much better on the data it was trained on than on held-out test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ldmvp28iEuRh"
   },
   "source": [
    "## Section 1.4: Train a random forest model on the testing data and evaluate it\n",
    "\n",
    "Use what you know to train a random forest model on the training data and evaluate it on both the training and test data.\n",
    "We have already imported `RandomForestRegressor` in Setup section via\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor  \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nbS9mZtWkvuK"
   },
   "outputs": [],
   "source": [
    "def train_random_forest_model(X_train, y_train, X_test, y_test, random_state):\n",
    "    \"\"\"Train a Random Forest model and evaluate its performance.\n",
    "\n",
    "    Args:\n",
    "        X_train (ndarray): Training features.\n",
    "        y_train (ndarray): Training labels.\n",
    "        X_test (ndarray): Test features.\n",
    "        y_test (ndarray): Test labels.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        RandomForestRegressor: Trained Random Forest regressor model.\n",
    "    \"\"\"\n",
    "    #################################################\n",
    "    ## TODO for students: Train a random forest model on the testing data and evaluate it ##\n",
    "    # Implement training a RandomForestRegressor model using X_train and y_train\n",
    "    # Then, evaluate its performance on both training and test data using .score() method\n",
    "    # Print out the performance on training and test data\n",
    "    # Please remove the following line of code once you have completed the exercise:\n",
    "    raise NotImplementedError(\"Student exercise: Implement the training and evaluation process.\")\n",
    "    #################################################\n",
    "\n",
    "    # Train the model on the training data\n",
    "    rf_regressor = RandomForestRegressor(random_state=random_state)\n",
    "\n",
    "    # fit the model\n",
    "    _ = rf_regressor.fit(..., ...)\n",
    "\n",
    "    print('Performance on training data :', rf_regressor.score(..., y_train))\n",
    "    print('Performance on test data     :', rf_regressor.score(X_test, ...))\n",
    "\n",
    "    return rf_regressor\n",
    "\n",
    "# test the function\n",
    "rf_model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qxifZbjzrlQ5",
    "outputId": "5f0a66b0-9ef7-4b8c-e2c9-ddaa8509e05d"
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "def train_random_forest_model(X_train, y_train, X_test, y_test, random_state):\n",
    "    \"\"\"Train a Random Forest model and evaluate its performance.\n",
    "\n",
    "    Args:\n",
    "        X_train (ndarray): Training features.\n",
    "        y_train (ndarray): Training labels.\n",
    "        X_test (ndarray): Test features.\n",
    "        y_test (ndarray): Test labels.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        RandomForestRegressor: Trained Random Forest regressor model.\n",
    "    \"\"\"\n",
    "\n",
    "    # train the model on the training data\n",
    "    rf_regressor = RandomForestRegressor(random_state=random_state)\n",
    "    \n",
    "    # fit the model\n",
    "    _ = rf_regressor.fit(X_train, y_train)\n",
    "    \n",
    "    print('Performance on training data :', rf_regressor.score(X_train, y_train))\n",
    "    print('Performance on test data     :', rf_regressor.score(X_test, y_test))\n",
    "\n",
    "    return rf_regressor\n",
    "\n",
    "# test the function\n",
    "rf_model = train_random_forest_model(X_train, y_train, X_test, y_test, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNvkM8PYem67"
   },
   "source": [
    "### Question 1.4: Overfitting - Decision Tree vs Random Forest\n",
    "\n",
    "1. Does the random forest model overfit less than a single decision tree?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "YqBbuUXbsC5c",
    "outputId": "0bac62ec-6e13-449b-c339-e239e9401860"
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "1. The difference between performance on training and test data is less for the random forest model therefore it overfits less. \n",
    "This is consistent with what we learned about the benefit of using ensemble models. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSz7m4ybQtnC"
   },
   "source": [
    "## Section 1.5: Explore Parameters of the Random Forest Model\n",
    "\n",
    "In the previous tutorial, you saw how we can control the depth of a single decision tree.   \n",
    "We can also control the depth of the decision trees used in our random forest model by passing a `max_depth` argument. We can also control the number of trees in the random forest model by setting `n_estimator`.\n",
    "\n",
    "Intuitively, these variables control the *capacity* of the model. Capacity loosely refers to the number of trainable parameters in the model. The more trees and the deeper they are, the more free parameters the model has to capture the training data. If the model has too low of capacity, it won't be powerful enough to capture complex relationships between the input features and the target variable. If it has too many parameters that it can move around, however, it may end up memorizing every single training point and therefore overfit.\n",
    "\n",
    "Use the sliders below to experiment with different values of `n_estimator` and `max_depth` and see how they impact performance on training and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTAv3t1Azi83"
   },
   "source": [
    "### Interactive Demo 1.5:  Performance of the Random Forest Regression\n",
    "In this activity, you can adjust the sliders for `n_estimators` and `max_depth` to observe their effect on model performance:\n",
    "\n",
    "* `n_estimators`: Controls the number of trees in the Random Forest.   \n",
    "* `max_depth`: Sets the maximum depth of each tree.  \n",
    "After adjusting the sliders, the code fits a new Random Forest model and prints the training and testing scores, showing how changes in these parameters impact model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Use the slider to change the values of 'n_estimators' and 'max_depth' and observe the effect on performance.\n",
    "# @markdown Make sure you execute this cell to enable the widget!\n",
    "\n",
    "# Function to train random forest and display scatter plot\n",
    "def train_rf_and_plot(X_tr, y_train, X_test, y_test, max_depth, n_estim):\n",
    "    global rf_regressor, X_train\n",
    "\n",
    "    # Instantiate and train the decision tree regressor\n",
    "    rf_regressor = RandomForestRegressor(n_estimators=n_estim, max_depth=max_depth)\n",
    "    rf_regressor.fit(X_tr, y_train)\n",
    "\n",
    "    # Calculate and print the scores\n",
    "    score_train = rf_regressor.score(X_tr, y_train)\n",
    "    score_test = rf_regressor.score(X_test, y_test)\n",
    "    print(f\"\\n\\tTraining Score: {score_train}\")\n",
    "    print(f\"\\tTesting Score  : {score_test}\\n\")\n",
    "    \n",
    "    # Generate scatter plot: Predicted vs. True Temperatures\n",
    "    predicted = rf_regressor.predict(X_tr)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Scatter plot\n",
    "    ax.scatter(predicted, y_train, color='blue', alpha=0.7, label='Comparison of Predicted and True Temperatures', edgecolors='black')\n",
    "    ax.plot([min(y_train), max(y_train)], [min(y_train), max(y_train)], color='red', linestyle='--', label='Ideal Prediction Line')\n",
    "    ax.set_xlabel('Predicted Temperature (K)')\n",
    "    ax.set_ylabel('True Temperature (K)')\n",
    "    ax.set_title('Annual mean temperature anomaly')\n",
    "    # add a caption \n",
    "    caption_text = 'The anomalies are calculated by subtracting the annual means of the pre-industrial scenario from \\nthe annual means of the respective scenario.'\n",
    "    plt.figtext(0.5, -0.03, caption_text, ha='center', fontsize=10)  # Adjusted y-coordinate to create space\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Interactive widget to control max_depth and n_estimators\n",
    "@widgets.interact(max_depth=(1, 41, 1), n_estimators=(10,100,5))\n",
    "def visualize_scores_with_max_depth(max_depth=20, n_estimators=50):\n",
    "    train_rf_and_plot(X_train, y_train, X_test, y_test, max_depth, n_estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JhT6eg0QM2x"
   },
   "source": [
    "### Interactive Demo 1.5: Discussion\n",
    "\n",
    "1. Did you observe any trends in how the performance changes?  \n",
    "2. Try to explain in you own words the concepts of capacity and overfitting and how they relate.\n",
    "3. In addition to model capacity, what else could be changed to prevent overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "tizaDDH3hbA0",
    "outputId": "406779f5-943f-4efb-edda-b81d4bdbe5d8"
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "1. Observations: Adjusting `n_estimators` and `max_depth` may cause fluctuations in model performance. Increasing `n_estimators` initially improves performance, but too many trees may lead to overfitting. Similarly, increasing `max_depth` initially enhances performance by capturing complex patterns, but excessively deep trees may result in overfitting.\n",
    "\n",
    "2. Capacity and Overfitting: Capacity refers to a model's ability to capture complex patterns, while overfitting occurs when a model learns noise instead of true patterns. Increasing capacity, like using more trees or deeper trees, can lead to overfitting.\n",
    "\n",
    "3. Preventing Overfitting: Apart from adjusting model capacity, we could also consider training on a larger dataset. Machine learning techniques like regularization techniques, cross-validation, feature selection, and ensemble methods help prevent overfitting. These approaches ensure the model generalizes well to unseen data by balancing complexity and performance.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7VAbXQxYiNP"
   },
   "source": [
    "# Summary\n",
    "\n",
    "In this tutorial, we delved into the importance of training and testing sets in constructing robust machine learning models. Understanding the concept of overfitting and the necessity of using separate test sets for model assessment were pivotal. Through practical exercises, we acquired hands-on proficiency in data partitioning, model training, and performance evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SpKyVbrKzHe"
   },
   "source": [
    "# Resources\n",
    "\n",
    "* [ClimateBench v1.0: A Benchmark for Data-Driven Climate Projections](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2021MS002954) \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
