{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuromatch/climate-course-content/blob/main/tutorials/W2D5_ClimateResponse-AdaptationImpact/W2D5_Tutorial2.ipynb) Â  <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/climate-course-content/main/tutorials/W2D5_ClimateResponse-AdaptationImpact/W2D5_Tutorial2.ipynb\" target=\"_blank\"><img alt=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 2:  Building and Training Random Forest Models\n",
    "\n",
    "**Week 2, Day 5, AI and Climate Change**\n",
    "\n",
    "__Content creators:__  Deepak Mewada, Grace Lindsay\n",
    "\n",
    "__Content reviewers:__ Mujeeb Abdulfatai, Nkongho Ayuketang Arreyndip, Jeffrey N. A. Aryee, Paul Heubel, Jenna Pearson, Abel Shibu\n",
    "\n",
    "__Content editors:__ Deepak Mewada, Grace Lindsay\n",
    "\n",
    "__Production editors:__ Konstantine Tsafatinos\n",
    "\n",
    "**Our 2024 Sponsors:** CMIP, NFDI4Earth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial:* 35 minutes\n",
    "\n",
    "In this tutorial, you will    \n",
    "* Learn about decision trees and hyperparameters\n",
    "* Learn about random forest models\n",
    "* Understand how regression models are evaluated (R$^2$)\n",
    "* Familiarize yourself with the scikit-learn package\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt                           # For plotting graphs\n",
    "import pandas as pd                                       # For data manipulation\n",
    "import ipywidgets as widgets                              # interactive display\n",
    "from sklearn.ensemble import RandomForestRegressor        # For Random Forest Regression\n",
    "from sklearn.tree import DecisionTreeRegressor            # For Decision Tree Regression\n",
    "from sklearn.tree import plot_tree                        # For plotting decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure Settings\n",
    "import ipywidgets as widgets  # interactive display\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\n",
    "    \"https://raw.githubusercontent.com/neuromatch/climate-course-content/main/cma.mplstyle\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed=None):\n",
    "    if seed is None:\n",
    "        seed = np.random.choice(2 ** 32)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    print(f'Random seed {seed} has been set.')\n",
    "\n",
    "# Set a global seed value for reproducibility\n",
    "random_state = 42 # change 42 with any number you like\n",
    "\n",
    "set_seed(seed=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Building and training Random Forest Models\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "# curriculum or production team will provide these ids\n",
    "video_ids = [('Youtube', 'st_1ygEGQTQ'), ('Bilibili', '<video_id_2>'), ('Osf', '<video_id_3>')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "\n",
    "# @markdown\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import IFrame\n",
    "\n",
    "link_id = \"kyv6w\"\n",
    "\n",
    "download_link = f\"https://osf.io/download/{link_id}/\"\n",
    "render_link = f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/{link_id}/?direct%26mode=render%26action=download%26mode=render\"\n",
    "# @markdown\n",
    "out = widgets.Output()\n",
    "with out:\n",
    "    print(f\"If you want to download the slides: {download_link}\")\n",
    "    display(IFrame(src=f\"{render_link}\", width=730, height=410))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 1: Preparing the Data for Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In this video, we learned about:\n",
    "\n",
    "1. Using regression for prediction tasks, like the one we have.\n",
    "2. The conceptual understanding of decision trees and their regression capabilities.\n",
    "3. Random forests as an ensemble of decision trees.\n",
    "4. Training our model\n",
    "4. Measuring model performance.\n",
    "5. Utilizing the scikit-learn toolbox for regression tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 1.1: Loading the data\n",
    "\n",
    "Remember from the previous tutorial how we loaded the `training_data`?\n",
    "Let's again load the data here for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#Load Dataset\n",
    "url_Climatebench_train_val = \"https://osf.io/y2pq7/download\"\n",
    "training_data = pd.read_csv(url_Climatebench_train_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Next, we will prepare the data to train a model to predict temperature anomalies in 2050. Let's also remind ourselves of what the data contains:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 1.2: Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Check column names (assuming a pandas DataFrame)\n",
    "print(\"Column names:\")\n",
    "print(training_data.columns.tolist())  # List all column names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "First, we will drop the `scenario` column from the data as it is just a label, but will not be passed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "training_data.pop('scenario')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "As we can see, scenario is no longer in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "print(\"Column names:\")\n",
    "print(training_data.columns.tolist())  # List all column names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Next, we need to pull out our target variable (that is, the variable we want our model to predict). Here that is `tas_FINAL`, the temperature anomaly in 2050.  The anomalies in every case are calculated by subtracting the annual means of the pre-industrial scenario from the annual means of the respective scenario of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "target = training_data.pop('tas_FINAL')\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "*Note: we will need to repeat these preprocessing steps anytime we load this (or other) data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 2: Fit Decision Tree and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now we can train our models. As mentioned in the video, Decision Trees and Random Forest Models can both do regression. Specifically:\n",
    "\n",
    "***Decision Tree Regression***:   \n",
    "* Decision trees recursively partition the feature space into regions based on feature values to predict the target variable.\n",
    "* Each leaf node represents a prediction.\n",
    "* Single trees can be prone to capturing noise in the data (not what we want!).  \n",
    "\n",
    "***Random Forest Regression***:   \n",
    "* An ensemble method that combines multiple decision trees to improve predictive performance.\n",
    "* Each tree is trained on a random subset of the data.\n",
    "* Aggregates predictions of individual trees to improve performance.\n",
    "* Typically more robust/doesn't capture noise.\n",
    "\n",
    "We will see an example of both here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "First, let's train a single decision tree to try to predict 2050 temperature anomalies using 2015 temperature anomalies and emissions data. We can control the depth of our decision tree (which is the maximum number of splits performed), which we will set to 20 here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.0: Scikit-learn\n",
    "\n",
    "In this and coming sub-sections, we will utilize [Scikit-learn](https://scikit-learn.org/stable/), commonly referred to as `sklearn`, a renowned Python library extensively employed for machine learning endeavors. It provides a comprehensive array of functions and tools tailored for various machine learning tasks. Specifically, we will concentrate on the [`DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) and [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) modules offered by Scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.1: Training the Decision Tree and Analyzing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# instantiate the model:\n",
    "dt_regressor = DecisionTreeRegressor(random_state=random_state,max_depth=20)\n",
    "\n",
    "# fit/train the model with the data:\n",
    "dt_regressor.fit(training_data, target) #pass in the model inputs and the target it should predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We've trained our first model! Now let's see how well it performs. As discussed in the video, we will use the coefficient of determination (also known as the R-squared value, $R^2$) as the measure of how well the model is doing.\n",
    "\n",
    "We can get this value by calling the `score` function and providing the data we want the score calculated on. Here we will evaluate the model on the same data it was trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<details>\n",
    "<summary> <font color='lightGreen'>Learn more about the R-Squared value and Coefficient of determination   </font></summary>\n",
    "\n",
    "\n",
    " The **R-squared** value indicates the proportion of the variance in the target variable that is predicted from the model.\n",
    "\n",
    "Specifically, the ***coefficient of determination*** is calculated using the formula:\n",
    "\n",
    "$$\n",
    "\\color{#3182CE}{R^2} = 1 - \\frac{\\color{#DC3912}{SS_{\\text{residual}}}}{\\color{#FF9900}{SS_{\\text{total}}}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\color{#FF9900}{SS_{\\text{total}}}$ represents the total sum of squares, calculated as the sum of squared differences between the target variable $\\color{#2CA02C}{y}$ and its mean $\\color{#2CA02C}{\\bar{y}}$:\n",
    "\n",
    "$$\n",
    "\\color{#FF9900}{SS_{\\text{total}}} = \\sum_{i=1}^{n} (\\color{#2CA02C}{y_i} - \\color{#2CA02C}{\\bar{y}})^2\n",
    "$$\n",
    "\n",
    "- $\\color{#DC3912}{SS_{\\text{residual}}}$ denotes the residual sum of squares, computed as the sum of squared differences between the observed target values $\\color{#2CA02C}{y}$ and the predicted values $\\color{#FF5733}{\\hat{y}}$ provided by the model:\n",
    "\n",
    "$$\n",
    "\\color{#DC3912}{SS_{\\text{residual}}} = \\sum_{i=1}^{n} (\\color{#2CA02C}{y_i} - \\color{#FF5733}{\\hat{y}_i})^2\n",
    "$$\n",
    "\n",
    "The $\\color{#3182CE}{R^2}$ score thus quantifies the proportion of variance in the target variable that is predictable from the independent variables in the model.\n",
    "\n",
    "This value ranges from 0 to 1, where 1 indicates a perfect fit, meaning the model explains all the variability in the target variable.\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "dt_regressor.score(training_data, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "Now, let's create a scatter plot to compare the true temperature anomaly values in 2050 to those predicted by the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Scatter Plot: Predicted vs. True Temperatures for Decision Tree\n",
    "\n",
    "# Get predicted values\n",
    "predicted = dt_regressor.predict(training_data)\n",
    "\n",
    "# Create scatter plot\n",
    "plt.scatter(predicted, target, color='b', label='Comparison of Predicted and True Temperatures')\n",
    "plt.plot([0, 4], [0, 4], color='r', label='Ideal Line')  # Add a diagonal line for reference\n",
    "plt.xlabel('Predicted Temperatures (K)')\n",
    "plt.ylabel('True Temperatures (K)')\n",
    "plt.title('Annual mean temperature anomaly', fontsize=14)\n",
    "\n",
    "# Add a caption with adjusted y-coordinate to create space\n",
    "caption_text = 'The anomalies are calculated by subtracting the annual means of the pre-industrial scenario from \\nthe annual means of the respective scenario.'\n",
    "plt.figtext(0.5, -0.03, caption_text, ha='center', fontsize=10)  # Adjusted y-coordinate to create space\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<details>\n",
    "<summary> <font color='skyblue'>What can we conclude from this score and the scatter plot?<br>\n",
    "First, pause and think by yourself. Then, compare it with the information provided here:\n",
    "</font></summary>\n",
    "\n",
    "As we can see, the model achieves a high score of ~0.9984 on the training data. This indicates that the model can explain approximately 99.84% of the variance in the target variable based on the features in the training dataset. Such a high score suggests that the model fits the training data very well and can effectively capture the underlying patterns or relationships between the features and the target variable. We can see the close alignment between the true value and the value predicted by the model in the plot.\n",
    "\n",
    "However, it's essential to note that achieving a high score on the training data does not guarantee the model's performance on unseen data (i.e., the test or validation datasets). We will explore this more in the next tutorial.\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Interactive Demo 2.1: Variation in Performance with depth | Visualizing Decision Trees and Scatter plot\n",
    "\n",
    "In this interactive demo, we'll visualize decision trees using a widget. This widget enables interactive exploration of decision trees by adjusting two parameters:   \n",
    "`max_depth` controls the tree's complexity during training, while `dt_vis_depth` determines the depth of the tree to visualize. It dynamically trains a decision tree regressor based on `max_depth`, evaluates its performance with a scatter plot, and visualizes the tree structure up to `dt_vis_depth` using the plot_tree function.    \n",
    "This allows users to balance model complexity and interpretability, gaining insights into how different depths affect predictive accuracy and tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Make sure you execute this cell to enable the widget!\n",
    "# Don't worry about understanding this code! It's to set up an interactive plot.\n",
    "\n",
    "# Function to train decision tree and display scatter plot\n",
    "def train_and_plot(max_depth, visualize_depth):\n",
    "    global dt_regressor, training_data\n",
    "\n",
    "    # Instantiate and train the decision tree regressor\n",
    "    dt_regressor = DecisionTreeRegressor(max_depth=max_depth)\n",
    "    dt_regressor.fit(training_data, target)\n",
    "\n",
    "    # Calculate and print the score\n",
    "    score = dt_regressor.score(training_data, target)\n",
    "    print(f\"Model Score: {score}\")\n",
    "    print(f\"Please wait for ~{visualize_depth+visualize_depth/2} sec for the figure to render\")\n",
    "    # Generate scatter plot: Predicted vs. True Temperatures\n",
    "    predicted = dt_regressor.predict(training_data)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15+pow(1.3,visualize_depth), 6+pow(1.2,visualize_depth)), gridspec_kw={'width_ratios': [1, 1+visualize_depth/4]})\n",
    "\n",
    "    # Scatter plot\n",
    "    axes[0].scatter(predicted, target, color='blue', alpha=0.7, label='Comparison of Predicted and True Temperatures', edgecolors='black')\n",
    "    axes[0].plot([min(target), max(target)], [min(target), max(target)], color='red', linestyle='--', label='Ideal Prediction Line')\n",
    "    axes[0].set_xlabel('Predicted Temperature (K)', fontsize=12)\n",
    "    axes[0].set_ylabel('True Temperature (K)', fontsize=12)\n",
    "    axes[0].set_title('Annual mean temperature anomaly', fontsize=14)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # Decision tree visualization\n",
    "    plot_tree(dt_regressor, feature_names=training_data.columns, filled=True, fontsize=8, max_depth=visualize_depth, ax=axes[1])\n",
    "    axes[1].set_title(f'Decision Tree Visualization (Train_max_depth = {max_depth}, dt_visualize_depth = {visualize_depth})')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive widget to control max_depth\n",
    "@widgets.interact(max_depth=(1, 31, 1), dt_vis_depth=(1, 10, 1))\n",
    "def visualize_tree_with_max_depth(max_depth=20, dt_vis_depth=3):\n",
    "    train_and_plot(max_depth, dt_vis_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Interactive Demo 2.1 Discussion\n",
    "\n",
    "1. How does changing the max_depth parameter affect the decision tree's predictive accuracy and complexity?  \n",
    "\n",
    "2. What insights can be gained by visualizing the decision tree at different depths (dt_vis_depth)?\n",
    "\n",
    "3. What patterns or trends do you observe in the residuals (differences between predicted and true temperatures) on the scatter plot? How can these insights guide adjustments to improve the model's predictive accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "Discussion:\n",
    "1. Adjusting the `max_depth` parameter influences the complexity of the decision tree model. \n",
    "Increasing `max_depth` may lead to a more complex model that can capture intricate patterns in the training data, \n",
    "potentially resulting in higher predictive accuracy. (However, as we will discuss in the next tutorial, this can also increase the risk of overfitting, \n",
    "where the model learns noise in the training data instead of true patterns, leading to poor generalization to unseen data.)\n",
    "\n",
    "2. Visualizing the decision tree at different depths (`dt_vis_depth`) provides insights into the hierarchy of features \n",
    "and decision-making process within the model. Lower depths reveal high-level splits that capture broader patterns in the data, \n",
    "while higher depths expose finer details and nuances. By adjusting `dt_vis_depth`, \n",
    "users can focus on specific branches of the tree, uncovering key decision points and feature interactions. \n",
    "This exploration helps in understanding how the model makes predictions and identifying influential features in the dataset.\n",
    "\n",
    "3. By examining the scatter plot, we can identify any consistent patterns or trends in the residuals, indicating potential systematic errors \n",
    "or biases in the model's predictions. These observations can inform adjustments to the model, such as incorporating additional features \n",
    "or refining existing ones, to enhance its accuracy. Identifying outliers \n",
    "or clusters of residuals also highlights areas where the model may struggle to generalize, \n",
    "suggesting targeted improvements for better performance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.2: Training the Random forest and Analyzing the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now we will train an ensemble of decisions trees, known as a random forest. For this we can use the built-in `RandomForestRegressor` from the [sklearn.ensemble.RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html), which we have already imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "rf_regressor = RandomForestRegressor(random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The line of code creates a random forest regressor object named `rf_regressor`. This regressor is configured to use a specified `random_state` parameter, ensuring that the random number generation process within the algorithm is consistent across different runs. This helps maintain reproducibility in our experiments and ensures consistent results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now you will train the model on our data and calculate its score on the same data. Create a plot like the one above in order to visually inspect its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 2.2: Model Training and Performance Visualization of Ranodm Forest\n",
    "\n",
    "In this exercise, you will train a random forest regressor model on your data and evaluate its performance by calculating its score on the same data. Additionally, you will create a scatter plot to visually inspect its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def fit_and_visualize_rf(training_data, target):\n",
    "    \"\"\"Fit a random forest regressor to the training data and visualize the results.\n",
    "\n",
    "    Args:\n",
    "        training_data (array-like): Input data for training the model.\n",
    "        target (array-like): Target variable for training the model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    #################################################\n",
    "    ## TODO for students: Fit the random forest regressor and visualize the results ##\n",
    "    # Remove the following line of code once you have completed the exercise:\n",
    "    raise NotImplementedError(\"Student exercise: Fit the random forest regressor and visualize the results.\")\n",
    "    #################################################\n",
    "    \n",
    "    # fit the random forest regressor to the training data\n",
    "    _ = ...\n",
    "    \n",
    "    # print the R-squared score of the model\n",
    "    print('...')\n",
    "\n",
    "    # predict the target variable using the trained model\n",
    "    predicted = rf_regressor.predict(training_data)\n",
    "\n",
    "    # Create scatter plot\n",
    "    plt.scatter(predicted,target,color='b',label='Comparison of Predicted and True Temperatures')\n",
    "    plt.plot([0,4],[0,4],color='r', label='Ideal Line') # add a diagonal line for reference\n",
    "    plt.xlabel('Predicted Temperatures (K)')\n",
    "    plt.ylabel('True Temperatures (K)')\n",
    "    plt.legend()\n",
    "    plt.title('Annual mean temperature anomaly')\n",
    "    # add a caption with adjusted y-coordinate to create space\n",
    "    caption_text = 'The anomalies are calculated by subtracting the annual means of the pre-industrial scenario from \\nthe annual means of the respective scenario.'\n",
    "    plt.figtext(0.5, -0.03, caption_text, ha='center', fontsize=10)  # adjusted y-coordinate to create space\n",
    "\n",
    "# test your function\n",
    "_ = fit_and_visualize_rf(training_data, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "def fit_and_visualize_rf(training_data, target):\n",
    "    \"\"\"Fit a random forest regressor to the training data and visualize the results.\n",
    "\n",
    "    Args:\n",
    "        training_data (array-like): Input data for training the model.\n",
    "        target (array-like): Target variable for training the model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # fit the random forest regressor to the training data\n",
    "    _ = rf_regressor.fit(training_data, target)\n",
    "\n",
    "    # print the R-squared score of the model\n",
    "    print(rf_regressor.score(training_data, target))\n",
    "\n",
    "    # predict the target variable using the trained model\n",
    "    predicted = rf_regressor.predict(training_data)\n",
    "\n",
    "    # Create scatter plot\n",
    "    plt.scatter(predicted,target,color='b',label='Comparison of Predicted and True Temperatures')\n",
    "    plt.plot([0,4],[0,4],color='r', label='Ideal Line') # add a diagonal line for reference\n",
    "    plt.xlabel('Predicted Temperatures (K)')\n",
    "    plt.ylabel('True Temperatures (K)')\n",
    "    plt.legend()\n",
    "    plt.title('Annual mean temperature anomaly')\n",
    "    # add a caption with adjusted y-coordinate to create space\n",
    "    caption_text = 'The anomalies are calculated by subtracting the annual means of the pre-industrial scenario from \\nthe annual means of the respective scenario.'\n",
    "    plt.figtext(0.5, -0.03, caption_text, ha='center', fontsize=10)  # adjusted y-coordinate to create space\n",
    "\n",
    "# test your function\n",
    "_ = fit_and_visualize_rf(training_data, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "It seems like our models are performing very well! Let's think a bit more in the next tutorial about what else we should do to evaluate our models...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Summary\n",
    "\n",
    "Estimated timing of tutorial: 35 minutes\n",
    "\n",
    "In this tutorial, we delved into Random Forest Models and their application in climate prediction. We gained an understanding of regression and how Random Forests combine decision trees to improve predictive accuracy. Through practical exercises, we learned how to evaluate model performance and implement Random Forests using tools like scikit-learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [ClimateBench v1.0: A Benchmark for Data-Driven Climate Projections](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2021MS002954) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W2D5_Tutorial2",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
