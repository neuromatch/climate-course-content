{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuromatch/climate-course-content/blob/main/tutorials/W2D5_ClimateResponse-AdaptationImpact/W2D5_Tutorial1.ipynb)   <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/climate-course-content/main/tutorials/W2D5_ClimateResponse-AdaptationImpact/W2D5_Tutorial1.ipynb\" target=\"_blank\"><img alt=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 1:  ClimateBench Dataset and How Machine Learning Can Help\n",
    "\n",
    "**Week 2, Day 5, AI and Climate Change**\n",
    "\n",
    "__Content creators:__ Deepak Mewada, Grace Lindsay\n",
    "\n",
    "__Content reviewers:__ Mujeeb Abdulfatai, Nkongho Ayuketang Arreyndip, Jeffrey N. A. Aryee, Paul Heubel, Jenna Pearson, Abel Shibu\n",
    "\n",
    "__Content editors:__ Deepak Mewada, Grace Lindsay\n",
    "\n",
    "__Production editors:__ Konstantine Tsafatinos\n",
    "\n",
    "**Our 2024 Sponsors:** CMIP, NFDI4Earth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "execution": {},
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial:* 25 minutes\n",
    "\n",
    "Today, you will work on a total of 6 short tutorials.  In Tutorial 1, you delve into the fundamentals, including discussions on climate model emulators and the ClimateBench dataset. You gain insights into Earth System Models (ESMs) and Shared Socioeconomic Pathways (SSPs), alongside practical visualization techniques for ClimateBench features. Tutorial 2 expands on these foundations, exploring decision trees, hyperparameters, and random forest models. You learn to evaluate regression models, focusing on the coefficient of determination (R$^2$), and gain hands-on experience implementing models using `scikit-learn`. Tutorial 3 shifts focus to mitigating overfitting in machine learning models. Here, you learn the importance of model generalization and acquire practical skills for splitting data into training and test sets. In Tutorial 4, you refine your understanding of model robustness, with emphasis on within-distribution generalization and testing model performance on similar data. Tutorial 5 challenges you to test our models on various types of out-of-distribution data, while also exploring the role of climate model emulators in climate science research. Finally, Tutorial 6 concludes the series by discussing practical applications of AI and machine learning in addressing climate change-related challenges, and introducing available resources and tools in the field of climate change AI.\n",
    "\n",
    "In this tutorial, you will\n",
    "* Learn about the basics of data science and machine learning.\n",
    "* Define “climate model emulators”.\n",
    "* Introduce the ClimateBench dataset.\n",
    "* Visualize features from this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt     # For plotting graphs\n",
    "import pandas as pd                 # For data manipulation\n",
    "import xarray as xr                 # For multidimensional data manipulation\n",
    "import seaborn as sns               # For advanced visualizations\n",
    "import cartopy.crs as ccrs          # for geospatial visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Figure Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Figure Settings\n",
    "import ipywidgets as widgets  # interactive display\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\n",
    "    \"https://raw.githubusercontent.com/neuromatch/climate-course-content/main/cma.mplstyle\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Set random seed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Executing `set_seed(seed=seed)` you are setting the seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed=None):\n",
    "    if seed is None:\n",
    "        seed = np.random.choice(2 ** 32)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    print(f'Random seed {seed} has been set.')\n",
    "\n",
    "# Set a global seed value for reproducibility\n",
    "random_state = 42 # change 42 with any number you like\n",
    "\n",
    "set_seed(seed=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Video 1: Machine Learning on ClimateBench data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Machine Learning on ClimateBench data\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "# curriculum or production team will provide these ids\n",
    "video_ids = [('Youtube', 'k1jrcheoWP8'), ('Bilibili', '<video_id_2>'), ('Osf', '<video_id_3>')]\n",
    "tab_contents = display_videos(video_ids, W=730, H=410)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "editable": true,
    "execution": {},
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown\n",
    "from IPython.display import IFrame\n",
    "from ipywidgets import widgets\n",
    "out = widgets.Output()\n",
    "with out:\n",
    "    print(f\"If you want to download the slides: https://osf.io/download//\")\n",
    "    display(IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io//?direct%26mode=render%26action=download%26mode=render\", width=730, height=410))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 1: ClimateBench Dataset and How Machine Learning Can Help\n",
    "\n",
    "Section Objectives:\n",
    "* Understand how machine learning can be helpful generally\n",
    "* Understand the climate model data we will be working with\n",
    "* Understand the concept of a climate model emulator\n",
    "* Learn how to explore the dataset\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 1.1:  About the ClimateBench dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The ClimateBench dataset offers a comprehensive collection of hypothetical climate data derived from sophisticated computer simulations (specifically, the NorESM2 model, available via CIMP6). It includes information on key climate variables such as temperature, precipitation, and diurnal temperature range. These values are collected by running simulations that represent the different Shared Socioeconomic Pathways (SSPs). Each pathway is associated with a different projected emissions profile over time. This data thus provides insights into how these climate variables may change in the future due to different emission scenarios. By utilizing this dataset, researchers can develop predictive models to better understand and anticipate the impacts of climate change, ultimately aiding in the development of effective mitigation strategies. Specifically, this data set is well-formatted for training *machine learning models*, which is exactly what you will do here.\n",
    "\n",
    "A brief overview of the ClimateBench dataset is provided below; for additional details, please refer to the full paper -\n",
    "\n",
    "[ClimateBench v1.0: A Benchmark for Data-Driven Climate Projections](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2021MS002954)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Spatial Resolution:\n",
    "The simulations are conducted on a grid with a spatial resolution of approximately 2°, allowing for analysis of regional climate patterns and phenomena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Variables:\n",
    "The dataset includes four main variables defined for each point on the grid:\n",
    "1. <font color='#1f77b4'>**Temperature (TAS)**</font>: Represents the annual mean surface air temperature.\n",
    "2. <font color='#ff7f0e'>**Diurnal Temperature Range (DTR)**</font>: Reflects the difference between the maximum and minimum temperatures within a day averaged annually.\n",
    "3. <font color='#2ca02c'>**Precipitation (PR)**</font>: Indicates the annual total precipitation.\n",
    "4. <font color='#d62728'>**90th Percentile of Precipitation (PR90)**</font>: Captures extreme precipitation events by identifying the 90th percentile of daily precipitation values.   \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### ScenarioMIP Simulations:\n",
    "The dataset incorporates ScenarioMIP simulations, exploring various future emission pathways under different socio-economic scenarios. Each scenario is defined by a set of annual emissions values over future years. We will look at 5 different scenarios in total here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Emissions Inputs:\n",
    "Emissions scenarios are defined according to the following four types of emissions:\n",
    "- <font color='#9467bd'>Carbon dioxide (CO<sub>2</sub>)</font> concentrations.\n",
    "- <font color='#8c564b'>Methane (CH<sub>4</sub>)</font> concentrations.\n",
    "- <font color='#e377c2'>Sulfur dioxide (SO<sub>2</sub>)</font> emissions, a precursor to sulfate aerosols.\n",
    "- <font color='#7f7f7f'>Black carbon (BC)</font> emissions.\n",
    "\n",
    "Note: In the ClimateBench dataset, sulfur dioxide and black carbon emissions are provided as a spatial map over grid locations, but we will just look at global totals here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Model Specifications:\n",
    "- Simulation Model: the NorESM2 model is run in its low atmosphere-medium ocean resolution (LM) configuration.\n",
    "- Model Components: Fully coupled earth system including the atmosphere, land, ocean, ice, and biogeochemistry components.\n",
    "- Ensemble Averaging: Target variables are averaged over three ensemble members to mitigate internal variability contributions.\n",
    "\n",
    "By leveraging the ClimateBench dataset, researchers gain insights into climate dynamics, enabling the development and evaluation of predictive models crucial for understanding and addressing climate change challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<p align='center'><img src='https://github.com/neuromatch/climate-course-content/blob/main/tutorials/static/W2D5_Tutorial1_climatebench_Scenario.png' alt='W2D5_Tutorial1_climatebench_Scenario'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "For simplicity's sake, we'll utilize a **condensed version of the ClimateBench dataset**. As mentioned above, we will be looking at only 5 scenarios ('SSPs', listed above as \"experiments\"), and all emissions will be given as global annual averages for the years 2015 to 2050. Furthermore, we will include climate variables for each spatial location (as defined by latitude and longitude for a restricted region) for the year 2015. The target for our model prediction will be temperature in the year 2050 for each spatial location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 1.2: Load the Dataset (Condensed Version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We will use `pandas` to interact with the data, which is shared in the `.csv` format. First, let us load the environmental data into a pandas dataframe and print its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#Load Dataset\n",
    "url_Climatebench_train_val = \"https://osf.io/y2pq7/download\"\n",
    "training_data = pd.read_csv(url_Climatebench_train_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 1.3: Explore Data Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Next, we will quickly explore the size of the data, check for missing data, and understand column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "print(training_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This tells us we have 3240 rows and 152 columns.\n",
    "\n",
    "Let's look at what these rows and columns mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Each row represents a combination of spatial location and scenario. The scenario can be found in the 'scenario' column while the location is given in the 'lat' and 'lon' columns. Climate variables for 2015 are given in the following columns and tas_FINAL represents the temperature in 2050. After these columns, we get the annual global emissions values for each of the 4 emissions types included in ClimateBench, starting in 2015 and ending in 2050."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Handle Missing Values (if necessary)**:\n",
    "\n",
    "We cannot train a machine learning model if there are values missing anywhere in this dataset. Therefore, we will check for missing values using `training_data.isnull().sum()`, which sums the number of 'null' or missing values. \n",
    "If missing values exist, we can consider imputation techniques (e.g., [`fillna`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html), [`interpolate`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html)) based on the nature of the data and the specific column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "training_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Here, there are no missing values as the sum of all [`isnull()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isnull.html) values is zero for all columns. So we are good to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 1.4: Visualize the data\n",
    "In this section, we'll utilize visualization techniques to explore the dataset, uncovering underlying patterns and distributions of the variables. Visualizations are instrumental in making informed decisions and conducting comprehensive data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Spatial Distribution of Temperature and Precipitation:**  \n",
    "Plotting the spatial distribution of temperature can reveal geographical patterns and hotspots. We will use the temperature at 2015, the starting point of our simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Create a xarray dataset from the pandas dataframe\n",
    "# for convenient plotting with cartopy afterwards\n",
    "ds = xr.Dataset({'tas_2015': ('points', training_data['tas_2015'])},\n",
    "                coords={'lon': ('points', training_data['lon']),\n",
    "                        'lat': ('points', training_data['lat'])}\n",
    "               )\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# create geoaxes\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# add coastlines\n",
    "ax.coastlines()\n",
    "\n",
    "# plot the data\n",
    "p = ax.scatter(ds['lon'], ds['lat'], c=ds['tas_2015'], cmap='coolwarm', transform=ccrs.PlateCarree())\n",
    "\n",
    "# add a colorbar\n",
    "cbar = plt.colorbar(p, orientation='vertical')\n",
    "cbar.set_label('Temperature (K)')\n",
    "\n",
    "# add a grid and labels\n",
    "ax.gridlines(draw_labels={\"bottom\": \"x\", \"left\": \"y\"})\n",
    "\n",
    "# add title\n",
    "plt.title('Spatial Distribution of\\nAnnual Mean Temperature anomalies (2015)\\n')\n",
    "\n",
    "# add a caption with adjusted y-coordinate to create space\n",
    "caption_text = 'The anomalies are calculated by subtracting the annual means of the pre-industrial scenario from \\nthe annual means of the respective scenario.'\n",
    "plt.figtext(0.5, -0.03, caption_text, ha='center', fontsize=10)  # Adjusted y-coordinate to create space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can see there are clear spatial variations in 2015 temperatures. Note the range of latitude and longitude values, this dataset does not cover the entire globe. In fact, it covers roughly the geographical region represented below:\n",
    "\n",
    "<p align='center'><img src='https://github.com/neuromatch/climate-course-content/blob/main/tutorials/static/W2D5_Tutorial1_map.png' alt='W2D5_Tutorial1_map'/></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now use the same plotting code to make a plot of the spatial distribution of total precipitation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 1.4:  Plotting Spatial Distribution of Total Precipitation\n",
    "\n",
    "In this exercise, you will complete the code to plot the spatial distribution of total precipitation. Use the provided plotting code as a template and replace the ellipses with appropriate values.\n",
    "\n",
    "*Note that you have the necessary libraries already imported* (`xarray`, `matplotlib.pyplot`, `cartopy.crs` *and* `pandas`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def plot_spatial_distribution(data, col_name, c_label):\n",
    "    \"\"\"\n",
    "    Plot the spatial distribution of a variable of interest.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame containing latitude, longitude, and data of interest.\n",
    "        col_name (str): Name of the column containing data of interest.\n",
    "        c_label (str): Label to describe quantity and unit for the colorbar labeling.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # create a xarray dataset from the pandas dataframe\n",
    "    # for convenient plotting with cartopy afterwards\n",
    "    ds = xr.Dataset({col_name: ('points', data[col_name])},\n",
    "                    coords={'lon': ('points', data['lon']),\n",
    "                            'lat': ('points', data['lat'])}\n",
    "                   )\n",
    "\n",
    "    # create geoaxes\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "    # add coastlines\n",
    "    ax.coastlines()\n",
    "\n",
    "    # plot the data\n",
    "    p = ax.scatter(..., ... ,... , cmap='coolwarm', transform=ccrs.PlateCarree())\n",
    "\n",
    "    # add a colorbar\n",
    "    cbar = plt.colorbar(p, orientation='vertical')\n",
    "    cbar.set_label(c_label)\n",
    "\n",
    "    # add a grid and labels\n",
    "    ax.gridlines(draw_labels={\"bottom\": \"x\", \"left\": \"y\"})\n",
    "\n",
    "    # add title\n",
    "    plt.title('Spatial Distribution of\\n Annual Mean Anomalies\\n')\n",
    "    plt.show()\n",
    "\n",
    "# test your function along precipitation data\n",
    "_ = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/neuromatch/climate-course-content/tree/main/tutorials/W2D5_ClimateResponse-AdaptationImpact/solutions/W2D5_Tutorial1_Solution_3f5694a6.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=445.0 height=575.0 src=https://raw.githubusercontent.com/neuromatch/climate-course-content/main/tutorials/W2D5_ClimateResponse-AdaptationImpact/static/W2D5_Tutorial1_Solution_3f5694a6_0.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Time Series Plot of Emissions Scenarios:**\n",
    "\n",
    "\n",
    "We will plot the time series of each of the four emissions scenarios in this dataset (we will get to the fifth one later). Each row in the dataset with the same 'scenario' label has the same emissions values over time. So we will only use the data from the first spatial location for each scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Run this cell to plot the Time Series Plot of Emissions Scenarios:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Run this cell to plot the Time Series Plot of Emissions Scenarios:\n",
    "# Don't worry about understanding this code! It's to set up the plot.\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Extract emissions data for each scenario\n",
    "CO2_data = training_data.filter(regex='CO2_\\d+')\n",
    "SO2_data = training_data.filter(regex='SO2_\\d+')\n",
    "CH4_data = training_data.filter(regex='CH4_\\d+')\n",
    "BC_data = training_data.filter(regex='BC_\\d+')\n",
    "\n",
    "# Define the four scenarios\n",
    "scenarios = ['ssp585',  'ssp370-lowNTCF','ssp126', 'ssp370',]\n",
    "\n",
    "# Create subplots for each emission gas\n",
    "fig, axs = plt.subplots(4, 1, figsize=(8, 15), sharex=True)\n",
    "\n",
    "# Define units for each emission\n",
    "units = {'CO2': 'GtCO2', 'CH4': 'GtCH4 / year', 'SO2': 'TgSO2 / year', 'BC': 'TgBC / year'}\n",
    "\n",
    "# Plot emissions data for each emission gas with enhanced styling\n",
    "for i, (data, emission) in enumerate(zip([CO2_data,  CH4_data, SO2_data,BC_data], ['CO2',  'CH4', 'SO2','BC'])):\n",
    "    # Plot each scenario for the current emission gas\n",
    "    for scenario in scenarios:\n",
    "        scenario_data = data[training_data['scenario'] == scenario]\n",
    "        axs[i].plot(range(2015, 2051), scenario_data.mean(axis=0), label=scenario)\n",
    "\n",
    "    # Set ylabel and title for the current emission gas\n",
    "    axs[i].set_ylabel(f'{emission} Emissions ({units[emission]})', fontsize=12)\n",
    "    axs[i].set_title(f'{emission} Emissions', fontsize=14)\n",
    "    axs[i].legend()\n",
    "\n",
    "# Set common xlabel\n",
    "plt.xlabel('Time (years)')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show legends\n",
    "plt.legend()\n",
    "\n",
    "# Remove spines from all subplots\n",
    "for ax in axs:\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Customize ticks\n",
    "plt.xticks()\n",
    "plt.yticks()\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True, linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This last plot displays the global mean emissions contained in the ClimateBench dataset over the years 2015 to 2050 for four atmospheric constituents that are important for defining the forcing (cumulative anthropogenic carbon dioxide CO$_2$, methane CH$_4$, sulfur dioxide SO$_2$, black carbon BC). Each line represents a different emission scenario, which shows us trends and variations in emissions over time. The 'ssp370-lowNTCF' refers to a variation of the ssp370 scenario which includes lower emissions of near-term climate forcers (NTCFs) such as aerosol (but not methane). \n",
    "These emission scenarios are used in the following tutorials as features/predictors for our prediction of the temperature in 2050.\n",
    "\n",
    "All time series are derived from NorESM2 ScenarioMIP simulations available. Please read the paper of [Watson-Parris et al. (2022)](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2021MS002954) for a more detailed explanation of the ClimateBench dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Summary\n",
    "\n",
    "In this tutorial, you acquainted yourself with the ClimateBench dataset and explored how machine learning contributes to climate analysis. We defined the versatility of machine learning and its role in predicting climate variables. By delving into the ClimateBench dataset, we highlight its accessibility in providing climate model data. We emphasize the importance of data visualization and engage in practical exercises to explore the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "* [ClimateBench v1.0: A Benchmark for Data-Driven Climate Projections](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2021MS002954) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W2D5_Tutorial1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}